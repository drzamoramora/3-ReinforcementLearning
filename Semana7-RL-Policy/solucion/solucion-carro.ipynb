{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MountainCarContinuous v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge gym\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red Neuronal\n",
    "\n",
    "- Recibe 2 inputs [car position, car velocity]\n",
    "- valor:Push car to the left (negative value) or to the right (positive value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo que predice el proximo paso. segun lo observado.\n",
    "n_inputs = 2 # [car position, car velocity]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(10, activation=\"elu\", input_shape=[n_inputs]),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE Utility Functions\n",
    "\n",
    "**tf.GradientTape()**: TensorFlow proporciona la API tf.GradientTape para la diferenciación automática; es decir, calcular el gradiente de un cálculo con respecto a algunas entradas, normalmente tf.Variables. TensorFlow \"registra\" las operaciones relevantes ejecutadas dentro del contexto de un tf.GradientTape en una \"cinta\". Luego, TensorFlow usa esa cinta para calcular los gradientes de un cálculo \"grabado\" mediante la diferenciación en modo inverso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016000000000000004\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def calculate_reward(y_actual):\n",
    "    return math.pow(y_actual, 2) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# se calcula la probabilidad de ir a la izquierda.\n",
    "# se calcula el gradiente\n",
    "# se ejecuta un paso de env.step(...)\n",
    "# return: se devuelve lo que genero el paso (observaciones) y el gradiente actual.\n",
    "\n",
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_proba = model(obs[np.newaxis]) # dada una muestra, la probabilidad de ir a la izquierda\n",
    "        action = (tf.random.uniform([1,1]) > left_proba) # probabilidad aleatoria contra left_proba\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32) # prob left = (1-action)\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_proba)) # calcular la pérdida del paso actual.\n",
    "        grads = tape.gradient(loss, model.trainable_variables) # almacenar gradiente en grads. (usar más tarde)        \n",
    "    obs, reward, done, info = env.step([action[0,0].numpy()]) # juega la acción y obtén una nueva observación.\n",
    "    reward = calculate_reward(obs[0])\n",
    "    return obs, reward, done, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# un episodio es un epoch\n",
    "# un episodio se compone de varios steps.\n",
    "# vamos a calcular los rewards del step actual.\n",
    "# vamos a acumular los rewards\n",
    "# vamos a acumular los gradientes.\n",
    "# return: se devuelve todos los rewards acumuladors y todos los gradientes\n",
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn) # se ejecuta un step.\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done:\n",
    "                break;\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    return all_rewards, all_grads\n",
    "\n",
    "# esto devuelve una lista de recompensas por episodio y una lista de gradientes por episodio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recordemos que debemos descontar gamma (discount_factor) a los rewards\n",
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) -2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_factor\n",
    "    return discounted\n",
    "\n",
    "# se aplican los descuentos a los rewards y ademas se normalizan los datos.\n",
    "def discount_and_normalize(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor) for rewards in all_rewards] # aplica descuento\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std for discounted_rewards in all_discounted_rewards] # normalizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de hiper-parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 100\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 200\n",
    "discount_rate = 0.95\n",
    "optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "loss_fn = keras.losses.mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop de Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 99, mean rewards: 2.4"
     ]
    }
   ],
   "source": [
    "# creamos el ambiente en Open AI-Gym\n",
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "env.seed(42);\n",
    "\n",
    "# el main loop de entrenamiento\n",
    "for iteration in range(n_iterations):\n",
    "    \n",
    "    # Ejecutamos multiples episodios los cuales tienen varios steps\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "    \n",
    "    # acumula todos los rewards\n",
    "    total_rewards = sum(map(sum, all_rewards))                    \n",
    "   \n",
    "   \n",
    "\n",
    "    print(\"\\rIteration: {}, mean rewards: {:.1f}\".format(          \n",
    "        iteration, total_rewards / n_episodes_per_update), end=\"\")\n",
    "    \n",
    "    # los rewards se les aplica el descuento y se normalizan\n",
    "    all_final_rewards = discount_and_normalize(all_rewards,\n",
    "                                                       discount_rate)\n",
    "    \n",
    "    # se calcula la media ponderada de los gradientes para cada variable\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean(\n",
    "            [final_reward * all_grads[episode_index][step][var_index]\n",
    "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim\n",
    "\n",
    "def render_policy_net(model, n_max_steps=1500, seed=42):\n",
    "    frames = []\n",
    "    env = gym.make(\"MountainCarContinuous-v0\")\n",
    "    env.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    obs = env.reset()\n",
    "    for step in range(n_max_steps):\n",
    "        frames.append(env.render(mode=\"rgb_array\"))\n",
    "        left_proba = model.predict(obs.reshape(1, -1))\n",
    "        action = (tf.random.uniform([1,1]) > left_proba)\n",
    "        obs, reward, done, info = env.step([float(action[0,0].numpy())])\n",
    "        #print(obs)\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.animation.FuncAnimation at 0x2a46d8e2c88>"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = render_policy_net(model)\n",
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
