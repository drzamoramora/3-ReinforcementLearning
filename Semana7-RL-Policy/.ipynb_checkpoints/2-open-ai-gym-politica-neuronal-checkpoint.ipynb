{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Politicas Basadas en Gradientes\n",
    "\n",
    "### Busqueda de Politicas (Policy Search)\n",
    "\n",
    "- Una politica es un set de acciones que un agente debe ejecutar para maximizar los rewards a traves del tiempo.\n",
    "- El algoritmo de las politicas no debe ser necesariamente deterministico.\n",
    "- La idea es maximizar los rewards, sin importar el nivel de observacion del ambiente (parcialmente observable - totalmente observable)\n",
    "\n",
    "Pensemos en una Roomba:\n",
    "\n",
    "<img src=\"img/rl1.jpg\" />\n",
    "\n",
    "- la aspiradora robot se mueve solo adelante y atras\n",
    "- puede rotar g grados hacia la derecha o izquiera.\n",
    "- Si el algoritmo de politicas es probabilistico, entonces se puede decir que si la posibilida de moverse hacia adelante es p, entonces la probabilidad de moverse hacia atras es 1 - p. \n",
    "- si el angulo de rotacion es aleatorio, entonces se puede mover al angulo r+ o r-. \n",
    "- probablemente si ejecutamos la Roomba con esos parametros se va a mover de forma erratica, pero que importa!, lo que importa es si es efectiva la politica, osea, cuanto polvo recoge en 30 minutos.\n",
    "\n",
    "<img src=\"img/rl2.png\" />\n",
    "\n",
    "Si disenamos un algoritmo para entrenar nuestra roomba, vamos a tener que avergiguar 2 paramertos: la probabilidad p de moverse hacia adelante, y el angulo de rotacion r.\n",
    "\n",
    "Podemos usar fuerza bruta o bien la aleatoriedad para definir esos parametros y ver si se maximizan los rewards. Sin embargo esto no es solo ineficiente sino poco practico.\n",
    "\n",
    "El concepto de **policy search** es entonces ese esfuerzo que se hace para averiguar esos parametros en un **policy space**.\n",
    "\n",
    "Existen diversas formas de crear politicas:\n",
    "\n",
    "- algoritmos geneticos\n",
    "- algoritmos neuronales\n",
    "- usar tecnicas de optimizacion: uso de gradientes. las politicas que se consiguen utilizando gradientes son llamadas **policy gradients**\n",
    "\n",
    "### Politicas Neuronales\n",
    "\n",
    "El problema de la politca que especificamos en el notebook anterior, es que esta no aprende, solo reacciona basado en una condicion. Podriamos reemplazar esa politica por una red neuronal perceptron, que reciba de entrada un dense de (input_shape = 4) para los parametros observados y que esta devuelva un dense (1, activacion = sigmoid) que reprenta 1 = derecha, 0 = izquierda. \n",
    "\n",
    "El pseudo-codigo del modelo seria algo asi:\n",
    "\n",
    "model = sequential <br/>\n",
    "&nbsp;&nbsp;&nbsp;Dense(5, input_shape = [4]),<br/>\n",
    "&nbsp;&nbsp;&nbsp;Dense(1, activation = sigmoid)\n",
    "\n",
    "Ahora bien, para entrenar esta red neuronal, podriamos que generar acciones aletorias con una probabilidad *p* de ir a la derecha y una probabilidad *1-p* de ir a la izquiera. Esto podria servir, principalmente porque permite una exploracion alatoria del ambiente. Sin embargo esta politica asi solita, no contempla el estado del paso anterior.\n",
    "\n",
    "Debido al problema de que RL solo juzga la politica escogida por el rewards acumulados, es dificl que pasos contribuyeron de forma positiva o negativa. Este problema se llama el \"Credit Assigment Problem\".\n",
    "\n",
    "Un ejemplo tipico es el del perro. Piense en un zaguate al que ud lleva todo el dia diciendole que no orine adentro de la casa. Cuando finalmente orina fuera de la casa, ud le da una galleta 10 minutos despues. Ahora sabe el perro porque lo estan premiando?\n",
    "\n",
    "### Credit Assigment Problem\n",
    "\n",
    "Una estrategia para lidiar con este problema consiste en evaluar la accion basado en la suma los premios que se generaron despues. Ademas de esto se aplica un factor de descuento (gamma $\\gamma$) en cada paso. La suma de las acciones con descuento se llaman el \"retorno\" (return) de la accion. Veamos un ejemplo:\n",
    "\n",
    "Si el agente de Pole decide ir a la derecha tres veces, este obtiene en el primer paso un premio de (+10), en el segundo paso (0) y en el tercer paso (-50), si se asume un factor de descuento $\\gamma$ = 0.8, la primera accion va a tener un retorno (return) de:\n",
    "\n",
    "- 10 + ($\\gamma$ * 0) + ($\\gamma^2$ * (-50)) = -22\n",
    "\n",
    "Si $\\gamma$ es cercano a cero, entonces los futuros rewards no van a contar mucho, solo los mas cercanos tendrian mas prevalencia. Normalmente se define $\\gamma$ en el rango de [0.9, 0.99] pero ud es bienvenido a explorar.\n",
    "\n",
    "El termino \"Action Advantage\" (AA) se refiere a que tan buena es una accion en comparacion con otras. Para estimar esto, debemos ejercutar varios episodios para estimar el AA. AL final debemos tener un buen balance de AA negativos y positivos para que el agente pueda aprender. \n",
    "\n",
    "Para lograr esto, vamos a desarrollar el algoritmo REINFORCE basado en politicas de gradientes. Esto lo vemos a continuacion.\n",
    "\n",
    "\n",
    "### Politicas basadas en Gradientes\n",
    "\n",
    "Algoritmo REINFORCE:\n",
    "\n",
    "1. Nuestra politica neuronal juega el juego varias veces. En cada step, se debe calcular los gradientes que harían que la acción elegida sea aún más probable, pero no se aplican los gradientes, solo se calculan.\n",
    "2. Despues de la ejecuacion de varios episodios, se calcula el \"Action Advantage\" de cada accion.\n",
    "3. Si el AA es bueno, entonces vamos a aplicar los gradientes para que la accion sea mas probable en el futuro. Si el AA es negativo, entonces se aplican inversamente los gradientes para bajar la probabilidad de escogencia de esa accion.\n",
    "4. Finalmente, se calcula la media de todos los gradientes y se aplica Gradient Descent.\n",
    "\n",
    "El siguiente codigo, es una implementacion con Keras y Tensorflow del Algoritmo REINFORCE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge gym\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red Neuronal\n",
    "\n",
    "- Recibe 4 inputs [car position, car velocity, pole angle, pole angular velocity]\n",
    "- devuelve 1/0 para izquierda o derecha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo que predice el proximo paso. segun lo observado.\n",
    "n_inputs = 4 #\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(5, activation=\"elu\", input_shape=[n_inputs]),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE Utility Functions\n",
    "\n",
    "**tf.GradientTape()**: TensorFlow proporciona la API tf.GradientTape para la diferenciación automática; es decir, calcular el gradiente de un cálculo con respecto a algunas entradas, normalmente tf.Variables. TensorFlow \"registra\" las operaciones relevantes ejecutadas dentro del contexto de un tf.GradientTape en una \"cinta\". Luego, TensorFlow usa esa cinta para calcular los gradientes de un cálculo \"grabado\" mediante la diferenciación en modo inverso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se calcula la probabilidad de ir a la izquierda.\n",
    "# se calcula el gradiente\n",
    "# se ejecuta un paso de env.step(...)\n",
    "# return: se devuelve lo que genero el paso (observaciones) y el gradiente actual.\n",
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_proba = model(obs[np.newaxis]) # dada una muestra, la probabilidad de ir a la izquierda\n",
    "        action = (tf.random.uniform([1,1]) > left_proba) # probabilidad aleatoria contra left_proba\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32) # prob left = (1-action)\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_proba)) # calcular la pérdida del paso actual.\n",
    "    grads = tape.gradient(loss, model.trainable_variables) # almacenar gradiente en grads. (usar más tarde)\n",
    "    obs, reward, done, info = env.step(int(action[0,0].numpy())) # juega la acción y obtén una nueva observación.\n",
    "    return obs, reward, done, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# un episodio es un epoch\n",
    "# un episodio se compone de varios steps.\n",
    "# vamos a calcular los rewards del step actual.\n",
    "# vamos a acumular los rewards\n",
    "# vamos a acumular los gradientes.\n",
    "# return: se devuelve todos los rewards acumuladors y todos los gradientes\n",
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn) # se ejecuta un step.\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done:\n",
    "                break;\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    return all_rewards, all_grads\n",
    "\n",
    "# esto devuelve una lista de recompensas por episodio y una lista de gradientes por episodio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-22 -40 -50]\n",
      "[array([-0.28435071, -0.86597718, -1.18910299]), array([1.26665318, 1.0727777 ])]\n"
     ]
    }
   ],
   "source": [
    "# Recordemos que debemos descontar gamma (discount_factor) a los rewards\n",
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) -2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_factor\n",
    "    return discounted\n",
    "\n",
    "# se aplican los descuentos a los rewards y ademas se normalizan los datos.\n",
    "def discount_and_normalize(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor) for rewards in all_rewards] # aplica descuento\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std for discounted_rewards in all_discounted_rewards] # normalizacion\n",
    "\n",
    "# prueba rapida\n",
    "print(discount_rewards([10, 0, -50], discount_factor = 0.8))\n",
    "print(discount_and_normalize([[10, 0, -50],[10,20]], discount_factor = 0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de hiper-parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 150\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 200\n",
    "discount_rate = 0.95\n",
    "optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "loss_fn = keras.losses.binary_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop de Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Iteration: 149, mean rewards: 178.3"
     ]
    }
   ],
   "source": [
    "# creamos el ambiente en Open AI-Gym\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env.seed(42);\n",
    "\n",
    "# el main loop de entrenamiento\n",
    "for iteration in range(n_iterations):\n",
    "    \n",
    "    # Ejecutamos multiples episodios los cuales tienen varios steps\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "    \n",
    "    # acumula todos los rewards\n",
    "    total_rewards = sum(map(sum, all_rewards))                    \n",
    "    \n",
    "    print(\"\\rIteration: {}, mean rewards: {:.1f}\".format(          \n",
    "        iteration, total_rewards / n_episodes_per_update), end=\"\")\n",
    "    \n",
    "    # los rewards se les aplica el descuento y se normalizan\n",
    "    all_final_rewards = discount_and_normalize(all_rewards,\n",
    "                                                       discount_rate)\n",
    "    \n",
    "    # se calcula la media ponderada de los gradientes para cada variable\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean(\n",
    "            [final_reward * all_grads[episode_index][step][var_index]\n",
    "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim\n",
    "\n",
    "def render_policy_net(model, n_max_steps=500, seed=42):\n",
    "    frames = []\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    env.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    obs = env.reset()\n",
    "    for step in range(n_max_steps):\n",
    "        frames.append(env.render(mode=\"rgb_array\"))\n",
    "        left_proba = model.predict(obs.reshape(1, -1))\n",
    "        action = (tf.random.uniform([1,1]) > left_proba)\n",
    "        obs, reward, done, info = env.step(int(action[0,0].numpy()))\n",
    "        #print(obs)\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.animation.FuncAnimation at 0x299a73e65f8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = render_policy_net(model)\n",
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
