{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cadena de Markov\n",
    "\n",
    "<img src=\"img/q1.jpeg\" />\n",
    "\n",
    "\n",
    "Además de usar políticas y gradientes, también podemos explicar el comportamiento de un agente como un grafo; mejor aún, como una cadena de Márkov.\n",
    "\n",
    "El siguiente dibujo establece un escenario básico donde un agente puede pasar de un estado s0 a un estado s1 a través de una acción a2 que tiene una probabilidad de 0.2… como podemos observar las transiciones entre estados se ejecutan mediante acciones que están sujetas a una probabilidad y además, algunas transiciones tienen rewards (negativos y positivos).\n",
    "\n",
    "El siguiente código representa las probabilidades de las transiciones como una matriz: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: 0 0 3 \n",
      "States: 0 1 2 1 2 1 2 1 2 1 3 \n",
      "States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
      "States: 0 3 \n",
      "States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
      "States: 0 1 3 \n",
      "States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 ...\n",
      "States: 0 0 3 \n",
      "States: 0 0 0 1 2 1 2 1 3 \n",
      "States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "transition_probabilities = [ # shape=[s, s']\n",
    "        [0.7, 0.2, 0.0, 0.1],  # from s0 to s0, s1, s2, s3\n",
    "        [0.0, 0.0, 0.9, 0.1],  # from s1 to ...\n",
    "        [0.0, 1.0, 0.0, 0.0],  # from s2 to ...\n",
    "        [0.0, 0.0, 0.0, 1.0]]  # from s3 to ...\n",
    "\n",
    "n_max_steps = 50\n",
    "\n",
    "def print_sequence():\n",
    "    current_state = 0\n",
    "    print(\"States:\", end=\" \")\n",
    "    for step in range(n_max_steps):\n",
    "        print(current_state, end=\" \")\n",
    "        if current_state == 3:\n",
    "            break\n",
    "        current_state = np.random.choice(range(4), p=transition_probabilities[current_state])\n",
    "    else:\n",
    "        print(\"...\", end=\"\")\n",
    "    print()\n",
    "\n",
    "for _ in range(10):\n",
    "    print_sequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP: Proceso de Desicion de Markov:\n",
    "\n",
    "Definamos algunas probabilidades de transición, recompensas y posibles acciones. Por ejemplo, en el estado s0, si se elige la acción a0 entonces con proba 0.7 pasaremos al estado s0 con recompensa +10, con probabilidad 0.3 pasaremos al estado s1 sin recompensa, y nunca pasaremos al estado s2 (entonces el las probabilidades de transición son [0.7, 0.3, 0.0] y las recompensas son [+10, 0, 0]) (similar a lo que vemos en el diagrama anterior):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities = [ # shape=[s, a, s']\n",
    "        [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "        [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
    "        [None, [0.8, 0.1, 0.1], None]]\n",
    "\n",
    "rewards = [ # shape=[s, a, s']\n",
    "        [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "        [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]]\n",
    "\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afortunadamente, podemos usar la función de optimalidad de Bellman (Bellman optimality Equation) para encontrar el estado mas optimo a partir de un estado  s.\n",
    "\n",
    "- $V^*(s) = max_a \\sum_s{T(s,a,s')[R(s,a,s')+\\gamma \\dot V^*(s')]} $ *para todo s*\n",
    "\n",
    "donde:\n",
    "\n",
    "- $T(s,a,s')$ es la probabilidad de transicion del estado s a s' dada una accion a.\n",
    "- $R(s,a,s')$ es el reward que gana el agente de ir del stado s a s'.\n",
    "- $\\gamma$ es el factor de descuento\n",
    "\n",
    "De esta formula, podemos dericar un algoritmo que permite estimar el estado optimo a partir de cada estado s. El algoritmo de Iteracion de Valor (Value Iteration Algorithm).  VAI sin embargo tiene un problema, a pesar de que puede encontrar el estado optimo s', este no necesariamente encuentra la mejor solucion en general.\n",
    "\n",
    "- $V^*_{k+1}(s) = max_a \\sum_s'{T(s,a,s')[R(s,a,s')+\\gamma \\dot V^*(s')]} $ *para todo s*\n",
    "\n",
    "Aqui $V^*_{k}(s)$ es el valor estimado del estado s en la iteracion k del algoritmo.\n",
    "\n",
    "Al notar este problema, Bellman encontro una solucion al problema al cual llamo Q-Values (Quality Values). El algoritmo es similar a VAI con algunas diferencias:\n",
    "\n",
    "- $Q_{k+1}(s,1) = \\sum_s'{T(s,a,s')[R(s,a,s')+\\gamma \\dot max_{a'} Q_k(s',a')]} $ *para todo s'a*\n",
    "\n",
    "Los estados optimos calculados por Q-Values, definen la politica basica que debe usar el agente. Por tanto si un agente esta en un estado s, la accion a utilizar esta definida por:\n",
    "\n",
    "- $\\pi^*(s)= argmax_a(Q^*(s,a))$\n",
    "\n",
    "Vamos a desarrollar un codigo basico para representar la generacion de los Q-Values.\n",
    "\n",
    "La política óptima para este MDP, cuando se usa un factor de descuento de 0.90, es elegir la acción a0 cuando está en el estado s0, y elegir la acción a0 cuando está en el estado s1, y finalmente elegir la acción a1 (la única acción posible) cuando está en el estado s2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   0.]\n",
      " [  0. -inf   0.]\n",
      " [-inf   0. -inf]]\n"
     ]
    }
   ],
   "source": [
    "Q_values = np.full((3, 3), -np.inf) # -np.inf for impossible actions\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0  # for all possible actions\n",
    "    \n",
    "print(Q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21.73304188 20.63807938 16.70138772]\n",
      " [ 0.95462106        -inf  1.01361207]\n",
      " [       -inf 53.70728682        -inf]]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.95  # the discount factor\n",
    "\n",
    "for iteration in range(50):\n",
    "    Q_prev = Q_values.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q_values[s, a] = np.sum([\n",
    "                    transition_probabilities[s][a][sp]\n",
    "                    * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp]))\n",
    "                for sp in range(3)])\n",
    "            \n",
    "print(Q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar Q-Values para determinar la proxima accion desde s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(Q_values, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Ahora la política ha cambiado! En el estado s1, ahora preferimos atravesar el fuego (elija la acción a2). Esto se debe a que el factor de descuento es mayor, por lo que el agente valora más el futuro y, por lo tanto, está listo para pagar una multa inmediata para obtener más recompensas futuras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n",
    "Hasta el momento, ha sido sencillo configurar el agente porque tenemos un diagrama de las probabilidades de transición, estados y rewards, sin embargo esto no sucede en la vida real. El agente debe por si solo, construir estas estructuras mediante prueba y error.\n",
    "\n",
    "Q-Learning funciona observando a un agente jugar (por ejemplo, de forma random) y mejorando gradualmente sus estimaciones de los Q-Values. Una vez que tiene estimaciones precisas de Q-Value (o lo suficientemente cerca), entonces la política óptima consiste en elegir la acción que tiene el Q-Value más alto .\n",
    "\n",
    "Necesitaremos simular un agente moviéndose en el entorno, así que definamos una función para realizar alguna acción y obtener el nuevo estado y una recompensa:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18.77621289 17.2238872  13.74543343]\n",
      " [ 0.                -inf -8.00485647]\n",
      " [       -inf 49.40208921        -inf]]\n"
     ]
    }
   ],
   "source": [
    "# esta funcion basicamente escoge el proximo estado s' de forma aleatoria \n",
    "# y devuelve el estado y el reward.\n",
    "def step(state, action):\n",
    "    probas = transition_probabilities[state][action]\n",
    "    next_state = np.random.choice([0, 1, 2], p=probas)\n",
    "    reward = rewards[state][action][next_state]\n",
    "    return next_state, reward\n",
    "\n",
    "# tambien necesitamos explorar todos los estados varas veces para obtener los estados s'.\n",
    "def exploration_policy(state):\n",
    "    return np.random.choice(possible_actions[state])\n",
    "\n",
    "# Ejecucion del algoritmo. \n",
    "# vamos a crear la tabla (matriz) de los Q-values\n",
    "np.random.seed(42)\n",
    "\n",
    "Q_values = np.full((3, 3), -np.inf)\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state][actions] = 0\n",
    "\n",
    "alpha0 = 0.05 # initial learning rate\n",
    "decay = 0.005 # learning rate decay\n",
    "gamma = 0.90 # discount factor\n",
    "state = 0 # initial state\n",
    "history2 = [] # Not shown in the book\n",
    "\n",
    "for iteration in range(10000):\n",
    "    history2.append(Q_values.copy())\n",
    "    action = exploration_policy(state)\n",
    "    next_state, reward = step(state, action)\n",
    "    next_value = np.max(Q_values[next_state]) # greedy policy at the next step\n",
    "    alpha = alpha0 / (1 + iteration * decay)\n",
    "    Q_values[state, action] *= 1 - alpha\n",
    "    Q_values[state, action] += alpha * (reward + gamma * next_value)\n",
    "    state = next_state\n",
    "\n",
    "history2 = np.array(history2)\n",
    "\n",
    "print(Q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accion optima para cada estado:\n",
    "np.argmax(Q_values, axis=1) # optimal action for each state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
