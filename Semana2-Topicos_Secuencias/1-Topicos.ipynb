{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelado de tópicos\n",
    "\n",
    "El modelado de tópicos es una tarea de NLP que busca, dado un *corpus* de documentos,\n",
    "\n",
    "1. Descubrir los distintos *tópicos* latentes en el *corpus*, y\n",
    "2. Entender cada documento como una combinación de distintos tópicos.\n",
    "\n",
    "Antes de explicar qué es un tópico, veamos el modelo de datos. Tenemos lo siguiente:\n",
    "\n",
    "1. Un cuerpo $D$ de $N$ documentos $d_1, \\ldots, d_N$\n",
    "2. Cada documento $d_i$ es una colección de palabras, donde $d_{i,j} \\ge 0$ es el número de veces que la palabra $w_j$ aparece en el documento $d_i$ (representación **bag of words**)\n",
    "3. Al conjunto total de $W$ palabras $w_1, w_2, \\ldots, w_W$ de todo el cuerpo se le conoce como **diccionario**.\n",
    "\n",
    "### Ejemplo mínimo\n",
    "\n",
    "**Corpus**\n",
    "> $d_1$ = ``Como poco coco como, poco coco compro. Como compro poco coco, poco coco como``<br>\n",
    "> $d_2$ = ``A Cuesta le cuesta subir la cuesta, y en medio de la cuesta va y se acuesta``\n",
    "\n",
    "**Diccionario**\n",
    "> ``como, poco, coco, compro, a, cuesta, le, subir, la, y, en, medio, de, va, se, acuesta`` <br>\n",
    "> $W = 16$\n",
    "\n",
    "**Representación bag-of-words**\n",
    "> $d_1$ = ``{\"coco\": 4, \"como\": 4, \"compro\": 2, \"poco\": 4}`` <br>\n",
    "> $d_2$ = ``{\"a\": 1, \"acuesta\": 1, \"cuesta\": 4, \"de\": 1, \"en\": 1, \"la\": 2, \"le\": 1, \"medio\": 1, \"se\": 1, \"subir\": 1, \"va\": 1, \"y\": 2}``\n",
    "\n",
    "Entonces, un tópico $t$ es una combinación lineal de palabras del diccionario, de modo que $\\sum_{j=1}^{m} t_j = 1$.\n",
    "\n",
    "\n",
    "En términos matriciales, la entrada de nuestro método será una matriz $\\mathbf{D}$ de dimension $N \\times W$. Esta matriz muy seguramente será **dispersa**. Es decir, que la mayoría de sus valores serán 0. \n",
    "\n",
    "||como|poco|coco|compro|a|cuesta|le|subir|la|y|en|medio|de|va|se|acuesta|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|$d_1$|4|4|4|2|0|0|0|0|0|0|0|0|0|0|0|0|\n",
    "|$d_2$|0|0|0|0|1|4|1|1|2|1|1|1|1|1|1|1|\n",
    "\n",
    "**Importante**: la información secuencial (cuál palabra sigue a cuál otra) se pierde en la representación *bag-of-words*. Luego estudiaremos otros modelos para esto. \n",
    "\n",
    "Finalmente, nuestro método de modelado de tópicos toma un valor $K$ (el número de tópicos a modelar) y va a producir lo siguiente:\n",
    "\n",
    "1. Una matriz de tópicos-palabras $T$ de dimensiones $K \\times W$\n",
    " - Es decir, de qué palabras se compone cada tópico (fila) $T_i$\n",
    " - De modo que $\\sum_{i=1}^W T_i = 1$\n",
    "\n",
    " \n",
    "2. Una matriz de documentos-tópicos $P$ de dimensiones $N \\times K$\n",
    " - Es decir, la fila $P_i$ nos explica de qué tópicos se compone el documento $d_i$\n",
    " - Igualmente, $\\sum_{i=1}^K P_i = 1$\n",
    "\n",
    "## Asignación Latente de Dirichlet (LDA)\n",
    "\n",
    "Uno de los mejores modelos de tópicos es la Asignación Latente de Dirichlet (LDA). \n",
    "\n",
    "Este modelo presenta una mejoría sobre métodos anteriores (como pLSA) en que es un modelo **bayesiano** que incorpora la siguiente suposición como información prior: **dentro de un *corpus*, la correspondencia entre documentos, palabras y tópicos será *dispersa***. \n",
    "\n",
    "Es decir, que un documento usualmente corresponde a **pocos tópicos**, y que los tópicos están compuestos de **pocas palabras**. Entonces, la gran mayoría de los elementos en las matrices $T$ y $P$ serán 0. \n",
    "\n",
    "### Dataset: 20 newsgroups\n",
    "\n",
    "Nuestro dataset de ejemplo será el conocido **20 newsgroups**. Este dataset contiene mensajes de 20 grupos de discusión de Usenet, divididos en varios tópicos, incluyendo política, religión, automovilismo, béisbol, computación, etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Grupos de discusión: \n",
      "\n",
      "alt.atheism\n",
      "comp.graphics\n",
      "comp.os.ms-windows.misc\n",
      "comp.sys.ibm.pc.hardware\n",
      "comp.sys.mac.hardware\n",
      "comp.windows.x\n",
      "misc.forsale\n",
      "rec.autos\n",
      "rec.motorcycles\n",
      "rec.sport.baseball\n",
      "rec.sport.hockey\n",
      "sci.crypt\n",
      "sci.electronics\n",
      "sci.med\n",
      "sci.space\n",
      "soc.religion.christian\n",
      "talk.politics.guns\n",
      "talk.politics.mideast\n",
      "talk.politics.misc\n",
      "talk.religion.misc\n",
      "\n",
      "\n",
      "*** Mensajes de ejemplo: \n",
      "\n",
      " \n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      " \n",
      "\n",
      "---\n",
      "\n",
      " \n",
      "\n",
      "WHen trying to choose a resistor with a tolerance better than 1%, you \n",
      "need a trimmer or to screen devices, it can't be made from adding 2 \n",
      "resitors of 1% value in parallel, since the smaller device will have the \n",
      "error of 1% to cope with. \n",
      "You have 3 choices;\n",
      "a) live with the error of 1% tolerance devices for low Q circuits or low \n",
      "sensitivity designs\n",
      "b) buy resistors with better than 1% tolerance (Vishay/Dale)\n",
      "c) use trimmers or SOT's (Select-On-Test)\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos el dataset \n",
    "import sklearn.datasets\n",
    "tng = sklearn.datasets.fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Vemos el listado de grupos de discusión\n",
    "print('*** Grupos de discusión: \\n')\n",
    "print(*tng['target_names'], sep='\\n')\n",
    "\n",
    "# Vemos un mensaje de ejemplo\n",
    "print('\\n\\n*** Mensajes de ejemplo: \\n\\n', tng['data'][0], '\\n\\n---\\n\\n', tng['data'][50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Preparación de datos\n",
    "\n",
    "Uno de los aspectos más importantes a la hora de trabajar con un dataset para análisis de lenguaje natural es el preprocesamiento, el cual incluye la eliminación de información irrelevante como signos de puntuación, diferencias entre mayúsculas y minúsculas y **\"stop words\"**, es decir, palabras como conjunciones (\"to\", \"a\", \"it\"). La clase que utilizaremos para convertir los documentos en su representación *bag-of-words*, llamada ``sklearn.feature_extraction.text.CountVectorizer``, trae consigo la facilidad para eliminar stop words y símbolos de puntuación. Veamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** ANTES:  \n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n",
      "*** DESPUES:\n",
      "\n",
      " sure bashers pens fans pretty confused lack kind posts recent pens massacre devils actually bit puzzled bit relieved going end non pittsburghers relief bit praise pens man killing devils worse thought jagr just showed better regular season stats lot fun watch playoffs bowman let jagr lot fun couple games pens going beat pulp jersey disappointed islanders lose final regular season game pens rule\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## Creamos una instancia de CountVectorizer usando stop words en inglés\n",
    "## y la \"entrenaremos\" con el primer documento para ver cómo resultan\n",
    "cv = CountVectorizer(stop_words='english', token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z][a-zA-Z]+\\b\").fit([tng['data'][0]])\n",
    "\n",
    "print('*** ANTES: ', tng['data'][0])\n",
    "print('*** DESPUES:\\n\\n',' '.join(cv.build_analyzer()(tng['data'][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, procederemos a realizar esto mismo con el dataset entero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## Tokenizamos el dataset entero (toma unos diez segundos en ejecutarse)\n",
    "cv = CountVectorizer(stop_words='english', token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z][a-zA-Z]+\\b\").fit(tng['data'])\n",
    "tng_matrix = cv.transform(tng['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutando LDA con ``scikit-learn``\n",
    "\n",
    "La clase de interés es ``sklearn.decomposition.LatentDirichletAllocation``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "## Entrenamos el modelo LDA (toma un par de minutos)\n",
    "lda = LatentDirichletAllocation(n_components=20, verbose=True).fit(tng_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** TOPICO  0\n",
      "['government' 'people' 'war' 'rights' 'state' 'states' 'turkish' 'right'\n",
      " 'public' 'united' 'national' 'law' 'security' 'american' 'privacy'\n",
      " 'armenian' 'information' 'new' 'world' 'muslims']\n",
      "\n",
      "\n",
      "*** TOPICO  1\n",
      "['gun' 'fbi' 'law' 'police' 'guns' 'people' 'koresh' 'don' 'crime'\n",
      " 'weapons' 'batf' 'government' 'think' 'firearms' 'control' 'use' 'like'\n",
      " 'just' 'did' 'illegal']\n",
      "\n",
      "\n",
      "*** TOPICO  2\n",
      "['game' 'team' 'games' 'year' 'play' 'season' 'hockey' 'players' 'win'\n",
      " 'league' 'player' 'good' 'baseball' 'don' 'period' 'pit' 'teams' 'think'\n",
      " 'hit' 'nhl']\n",
      "\n",
      "\n",
      "*** TOPICO  3\n",
      "['know' 'thanks' 'msg' 'food' 'mail' 'don' 'time' 'list' 'number' 'san'\n",
      " 'does' 'just' 'address' 'group' 'battery' 'looking' 'chinese' 'points'\n",
      " 'let' 'com']\n",
      "\n",
      "\n",
      "*** TOPICO  4\n",
      "['won' 'lost' 'mov' 'slave' 'use' 'period' 'health' 'drive' 'seattle'\n",
      " 'san' 'blue' 'number' 'master' 'new' 'rate' 'april' 'power' 'vancouver'\n",
      " 'red' 'national']\n",
      "\n",
      "\n",
      "*** TOPICO  5\n",
      "['people' 'don' 'just' 'think' 'like' 'say' 'know' 'does' 'way' 'point'\n",
      " 'good' 'time' 'make' 'really' 'did' 'right' 'want' 'believe' 'evidence'\n",
      " 'read']\n",
      "\n",
      "\n",
      "*** TOPICO  6\n",
      "['drug' 'drugs' 'ground' 'power' 'energy' 'current' 'voltage' 'science'\n",
      " 'research' 'like' 'japanese' 'circuit' 'university' 'carl' 'neutral'\n",
      " 'conductor' 'design' 'said' 'rockets' 'program']\n",
      "\n",
      "\n",
      "*** TOPICO  7\n",
      "['windows' 'dos' 'key' 'use' 'chip' 'using' 'encryption' 'know' 'clipper'\n",
      " 'like' 'keys' 'file' 'does' 'card' 'just' 'program' 'problem' 'message'\n",
      " 'time' 'don']\n",
      "\n",
      "\n",
      "*** TOPICO  8\n",
      "['like' 'car' 'just' 'drive' 'good' 'new' 'know' 'don' 'time' 'used'\n",
      " 'power' 'does' 'use' 'problem' 'bike' 'think' 'work' 'got' 'way' 'right']\n",
      "\n",
      "\n",
      "*** TOPICO  9\n",
      "['new' 'sale' 'theory' 'condition' 'shipping' 'art' 'cover' 'appears'\n",
      " 'man' 'book' 'universe' 'offer' 'wolverine' 'excellent' 'black' 'copies'\n",
      " 'larson' 'books' 'asking' 'price']\n",
      "\n",
      "\n",
      "*** TOPICO  10\n",
      "['window' 'use' 'server' 'motif' 'widget' 'edu' 'sun' 'set' 'file'\n",
      " 'available' 'application' 'mit' 'display' 'using' 'com' 'code' 'version'\n",
      " 'lib' 'xterm' 'subject']\n",
      "\n",
      "\n",
      "*** TOPICO  11\n",
      "['max' 'file' 'bhj' 'giz' 'entry' 'output' 'program' 'bxn' 'entries'\n",
      " 'printf' 'section' 'info' 'oname' 'build' 'rules' 'char' 'int' 'qax'\n",
      " 'okz' 'stream']\n",
      "\n",
      "\n",
      "*** TOPICO  12\n",
      "['edu' 'graphics' 'greek' 'pub' 'ray' 'mail' 'send' 'ftp' 'siggraph' 'com'\n",
      " 'list' 'file' 'site' 'objects' 'archive' 'greece' 'amiga' 'server'\n",
      " 'message' 'gov']\n",
      "\n",
      "\n",
      "*** TOPICO  13\n",
      "['armenian' 'armenians' 'turkey' 'university' 'azerbaijan' 'turkish'\n",
      " 'armenia' 'istanbul' 'professor' 'history' 'chz' 'ottoman' 'jews'\n",
      " 'population' 'new' 'genocide' 'russian' 'air' 'azeri' 'ankara']\n",
      "\n",
      "\n",
      "*** TOPICO  14\n",
      "['israel' 'israeli' 'arab' 'jews' 'jewish' 'edu' 'arabs' 'palestinian'\n",
      " 'palestinians' 'palestine' 'gaza' 'israelis' 'land' 'adam' 'objective'\n",
      " 'peace' 'lebanon' 'book' 'jerusalem' 'occupied']\n",
      "\n",
      "\n",
      "*** TOPICO  15\n",
      "['image' 'jpeg' 'images' 'file' 'gif' 'color' 'data' 'format' 'space'\n",
      " 'edu' 'files' 'bit' 'available' 'earth' 'use' 'version' 'nasa' 'software'\n",
      " 'display' 'ftp']\n",
      "\n",
      "\n",
      "*** TOPICO  16\n",
      "['god' 'jesus' 'people' 'bible' 'christian' 'church' 'christ' 'believe'\n",
      " 'does' 'christians' 'faith' 'say' 'know' 'lord' 'life' 'did' 'said' 'man'\n",
      " 'think' 'love']\n",
      "\n",
      "\n",
      "*** TOPICO  17\n",
      "['edu' 'said' 'com' 'know' 'didn' 'went' 'people' 'don' 'time' 'just'\n",
      " 'came' 'home' 'like' 'told' 'started' 'going' 'says' 'saw' 'say'\n",
      " 'children']\n",
      "\n",
      "\n",
      "*** TOPICO  18\n",
      "['think' 'space' 'don' 'people' 'president' 'going' 'know' 'time' 'like'\n",
      " 'just' 'make' 'money' 'said' 'new' 'year' 'did' 'years' 'stephanopoulos'\n",
      " 'work' 'government']\n",
      "\n",
      "\n",
      "*** TOPICO  19\n",
      "['software' 'use' 'edu' 'mac' 'data' 'computer' 'mail' 'scsi' 'like'\n",
      " 'thanks' 'available' 'need' 'does' 'email' 'know' 'new' 'card'\n",
      " 'information' 'bit' 'drive']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Y, ahora, vamos a ver cómo se ven los tópicos. \n",
    "import numpy as np\n",
    "comp = lda.components_\n",
    "vec = np.array(cv.get_feature_names())\n",
    "for i in range(0, comp.shape[0]):\n",
    "    print('*** TOPICO ', i)\n",
    "    print(vec[comp[i].argsort()[-20:][::-1]])\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Sale:\n",
      "\n",
      "Fujitsu 324meg SCSI drive.  $450\n",
      "\n",
      "Maxtor 338meg ESDI drive.  $425\n",
      "\n",
      "Maxtor 160meg ESDI drive.  $225\n",
      "\n",
      "Toshiba 106meg IDE drive.  $175\n",
      "\n",
      "XT case & motherboard.  $50\n",
      "\n",
      "DTC 16-bit MFM 2HD 2FD controler.  $30\n",
      "\n",
      "All items are used, in full working condition, and have a  \n",
      "warranty for one week unless otherwise specified.  All prices \n",
      "are %100 negotiable, shipping not included. \n",
      "\n",
      "Wanted:  \n",
      "\n",
      "Developers kit for SB\n",
      "17\" SVGA moniters (two of them).\n",
      "\n",
      "\n",
      "TOPICOS (en orden):\n",
      "[19  8  4  9  7 10 18 15  1 17  5  6 16  0  3  2 11 12 14 13]\n"
     ]
    }
   ],
   "source": [
    "## Ahora, veamos la composición de tópicos de algún documento\n",
    "\n",
    "print(tng['data'][1234])\n",
    "tx = lda.transform(tng_matrix[1234])\n",
    "\n",
    "print('TOPICOS (en orden):')\n",
    "print(tx.argsort()[0][::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
