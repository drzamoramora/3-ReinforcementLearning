{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea\n",
    "\n",
    "## Dataset: Correos de Enron\n",
    "\n",
    "Enron fue una empresa petrolera norteamericana que quebró estrepitosamente en el año 2001 luego de haber hecho lo que fue hasta su momento el fraude contable más grande de la historia. Como parte de la investigación criminal, se analizaron cientos de miles de mensajes de los servidores de correo electrónico de la compañía. \n",
    "\n",
    "El dataset está disponible para el público desde hace varios años. Aquí tenemos una muestra preprocesada con aproximadamente 15,000 mensajes de correo electrónico listos para ser analizados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos bibliotecas necesarias para la tarea\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from hmmlearn import hmm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Leemos el dataset\n",
    "enron_file = open('resources/enron.csv', 'r') \n",
    "enron = enron_file.readlines()\n",
    "enron = [ l.rstrip() for l in enron ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EJERCICIO 1 (1 punto)\n",
    "### - CONSTRUIR UN VECTORIZADOR DE CONTEOS QUE ELIMINE STOP-WORDS EN INGLÉS PARA EL DATASET enron\n",
    "### - UTILIZARLO PARA TRANSFORMAR EL DATASET EN UNA MATRIZ DOCUMENTO-PALABA\n",
    "cv = CountVectorizer(stop_words='english', token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z][a-zA-Z]+\\b\").fit(enron)\n",
    "enron_matrix = cv.transform(enron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n",
      "*** TOPICO  0\n",
      "['enron' 'know' 'group' 'business' 'new' 'information' 'risk' 'attached'\n",
      " 'gas' 'time' 'let' 'sally' 'team' 'office' 'questions' 'work' 'need'\n",
      " 'global' 'like' 'trading' 'energy' 'power' 'market' 'report' 'operations'\n",
      " 'thanks' 'issues' 'review' 'process' 'today']\n",
      "\n",
      "\n",
      "*** TOPICO  1\n",
      "['thanks' 'know' 'jeff' 'let' 'email' 'just' 'need' 'like' 'fyi' 'best'\n",
      " 'attached' 'week' 'going' 'sally' 'meeting' 'want' 'rick' 'list' 'think'\n",
      " 'time' 'image' 'mail' 'good' 'send' 'did' 'dont' 'sent' 'today' 'message'\n",
      " 'michelle']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### EJERCICIO 2 (4 puntos)\n",
    "### - ENTRENAR UN MODELO DE ASIGNACION LATENTE DE DIRICHLET SOBRE enron_matrix.\n",
    "### - MOSTRAR LOS TÓPICOS RESULTANTES (las 30 palabras más relevantes por tópico)\n",
    "### - PROBAR CON DISTINTOS NÚMEROS DE TÓPICOS, REPORTAR SUS RESULTADOS. \n",
    "lda = LatentDirichletAllocation(n_components=20, verbose=True).fit(enron_matrix)\n",
    "\n",
    "comp = lda.components_\n",
    "vec = np.array(cv.get_feature_names())\n",
    "for i in range(0, comp.shape[0]):\n",
    "    print('*** TOPICO ', i)\n",
    "    print(vec[comp[i].argsort()[-30:][::-1]])\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EJERCICIO 3 (1 punto)\n",
    "### - CONSTRUIR UN LabelEncoder CON LOS DATOS DE enron Y TRANSFORMAR LOS DATOS\n",
    "###   EL LabelEncoder DEBE SEPARAR LOS MENSAJES PALABRA POR PALABRA\n",
    "### - TRANSFORMAR EN DATASETS CODIFICADOS\n",
    "### - CREAR EL VECTOR DE LONGITUD DE MENSAJES\n",
    "###\n",
    "### *** IMPORTANTE: DADO EL TIEMPO EXCESIVO QUE TOMA ENTRENAR ESTE MODELO, \n",
    "###     SE RECOMIENDA TRABAJAR CON UNA MUESTRA REDUCIDA (p.ej. LOS PRIMEROS 100 MENSAJES)\n",
    "\n",
    "\n",
    "enron_chars = [ mail.lower().split(' ') for mail in enron[-200:] ]\n",
    "chars_len = [ len(chars) for chars in enron_chars ]\n",
    "cv_chars = LabelEncoder().fit([y for l in enron_chars for y in l])\n",
    "chars_enc = np.concatenate([cv_chars.transform(y).reshape(-1, 1) for y in enron_chars]).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting a model with 25579 free scalar parameters with only 12536 data points will result in a degenerate solution.\n",
      "         1      -98608.7151             +nan\n",
      "         2      -79855.4738      +18753.2413\n",
      "         3      -79851.0106          +4.4632\n",
      "         4      -79840.7529         +10.2577\n",
      "         5      -79817.9648         +22.7881\n",
      "         6      -79776.2895         +41.6753\n",
      "         7      -79720.0154         +56.2741\n",
      "         8      -79664.5340         +55.4814\n",
      "         9      -79620.0679         +44.4661\n",
      "        10      -79582.9114         +37.1565\n",
      "        11      -79545.0410         +37.8704\n",
      "        12      -79500.9616         +44.0794\n",
      "        13      -79447.7196         +53.2419\n",
      "        14      -79384.4173         +63.3024\n",
      "        15      -79308.8810         +75.5363\n",
      "        16      -79211.0179         +97.8630\n",
      "        17      -79076.1183        +134.8996\n",
      "        18      -78896.6547        +179.4636\n",
      "        19      -78680.0841        +216.5706\n",
      "        20      -78419.3249        +260.7592\n",
      "        21      -78087.4484        +331.8765\n",
      "        22      -77651.5985        +435.8499\n",
      "        23      -77084.0906        +567.5078\n",
      "        24      -76406.6344        +677.4562\n",
      "        25      -75671.7210        +734.9134\n",
      "        26      -74932.0292        +739.6918\n",
      "        27      -74248.2176        +683.8115\n",
      "        28      -73666.4705        +581.7471\n",
      "        29      -73188.2085        +478.2620\n",
      "        30      -72794.4263        +393.7822\n",
      "        31      -72483.4579        +310.9684\n",
      "        32      -72247.8417        +235.6162\n",
      "        33      -72062.6979        +185.1437\n",
      "        34      -71912.3408        +150.3571\n",
      "        35      -71790.3845        +121.9563\n",
      "        36      -71696.3674         +94.0171\n",
      "        37      -71623.0914         +73.2760\n",
      "        38      -71564.1242         +58.9672\n",
      "        39      -71512.8820         +51.2422\n",
      "        40      -71467.4823         +45.3998\n",
      "        41      -71428.1171         +39.3652\n",
      "        42      -71392.6337         +35.4834\n",
      "        43      -71362.7561         +29.8776\n",
      "        44      -71335.7660         +26.9902\n",
      "        45      -71313.5505         +22.2154\n",
      "        46      -71295.6818         +17.8688\n",
      "        47      -71279.6272         +16.0546\n",
      "        48      -71265.5691         +14.0581\n",
      "        49      -71253.5616         +12.0075\n",
      "        50      -71242.1654         +11.3962\n",
      "        51      -71230.3199         +11.8454\n",
      "        52      -71218.2633         +12.0567\n",
      "        53      -71206.0184         +12.2449\n",
      "        54      -71194.4028         +11.6155\n",
      "        55      -71185.1673          +9.2356\n",
      "        56      -71178.0841          +7.0832\n",
      "        57      -71171.8837          +6.2004\n",
      "        58      -71165.9692          +5.9145\n",
      "        59      -71160.3455          +5.6237\n",
      "        60      -71154.8352          +5.5103\n",
      "        61      -71149.3430          +5.4922\n",
      "        62      -71143.5673          +5.7757\n",
      "        63      -71136.7486          +6.8186\n",
      "        64      -71129.1438          +7.6049\n",
      "        65      -71122.1324          +7.0113\n",
      "        66      -71116.6237          +5.5087\n",
      "        67      -71107.2201          +9.4036\n",
      "        68      -71101.1841          +6.0360\n",
      "        69      -71097.2109          +3.9731\n",
      "        70      -71093.9757          +3.2352\n",
      "        71      -71090.9992          +2.9765\n",
      "        72      -71087.6844          +3.3149\n",
      "        73      -71084.0159          +3.6684\n",
      "        74      -71079.6366          +4.3793\n",
      "        75      -71074.0682          +5.5684\n",
      "        76      -71066.4904          +7.5778\n",
      "        77      -71058.2478          +8.2426\n",
      "        78      -71053.3728          +4.8750\n",
      "        79      -71050.4620          +2.9108\n",
      "        80      -71047.2961          +3.1659\n",
      "        81      -71043.1983          +4.0978\n",
      "        82      -71039.3637          +3.8346\n",
      "        83      -71036.2695          +3.0943\n",
      "        84      -71033.5452          +2.7243\n",
      "        85      -71031.0773          +2.4679\n",
      "        86      -71028.9626          +2.1147\n",
      "        87      -71026.7698          +2.1928\n",
      "        88      -71024.1917          +2.5781\n",
      "        89      -71020.5772          +3.6145\n",
      "        90      -71015.8107          +4.7665\n",
      "        91      -71011.6918          +4.1188\n",
      "        92      -71008.4871          +3.2047\n",
      "        93      -71006.0182          +2.4689\n",
      "        94      -71003.7549          +2.2633\n",
      "        95      -71001.6027          +2.1522\n",
      "        96      -70999.6101          +1.9926\n",
      "        97      -70997.6889          +1.9212\n",
      "        98      -70995.4182          +2.2707\n",
      "        99      -70992.7874          +2.6309\n",
      "       100      -70990.6537          +2.1337\n"
     ]
    }
   ],
   "source": [
    "### EJERCICIO 4 (4 puntos)\n",
    "### - ENTRENAR UN MODELO OCULTO DE MARKOV (HMM) SOBRE chars_enc Y chars_len\n",
    "### - GENERAR DIEZ SECUENCIAS ALEATORIAS DE DISTINTAS LONGITUDES\n",
    "### - PROBAR CON DISTINTOS NÚMEROS DE COMPONENTES, REPORTAR SUS RESULTADOS. \n",
    "\n",
    "enron_model = hmm.MultinomialHMM(n_iter=100, n_components=10, verbose=True).fit(chars_enc, chars_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from attached is jeff let counsel the course will an\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(cv_chars.inverse_transform(enron_model.sample(10)[0])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
