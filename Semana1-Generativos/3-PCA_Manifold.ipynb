{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA y Manifold learning\n",
    "\n",
    "Tomaremos un desvío de modelos generativos para entender un componente importante. El término *manifold learning* se utiliza para técnicas de reducción de dimensionalidad *no lineales* que parten de PCA y van *más allá*. \n",
    "\n",
    "Vamos a revisar detalles importantes del funcionamiento de PCA que nos servirán más adelante. \n",
    "\n",
    "Primero, el dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargamos el dataset de los pingüinos\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "penguins = fetch_openml('penguins')\n",
    "\n",
    "# Suprimimos la columna 'island' y las filas con NaNs\n",
    "X = penguins['data'][:,1:6]\n",
    "lab = penguins['target'][~np.any(np.isnan(X), axis=1)]\n",
    "X = X[~np.any(np.isnan(X), axis=1)]\n",
    "print(penguins['feature_names'])\n",
    "print(X.shape)\n",
    "pd.DataFrame(X, columns=penguins['feature_names'][1:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisión de PCA\n",
    "\n",
    "La idea de PCA es encontrar una proyección de un dataset $\\mathbf{X}$ con $n$ filas y $p$ columnas, hacia un dataset *de componentes principales* $\\mathbf{P}$, también de $n$ filas y $p$ columnas. Esta transformación es lineal: $\\mathbf{P} = \\mathbf{XR}$ siendo $\\mathbf{R}$ la denominada *matriz de rotación* o *de ejes principales*, con $p$ filas y $p$ columnas. \"Entrenar\" PCA consiste en conseguir los valores de $\\mathbf{R}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Entrenamos PCA sin reducir\n",
    "X_s = StandardScaler().fit_transform(X)\n",
    "pca = PCA()\n",
    "pca.fit(X_s)\n",
    "\n",
    "# Obtenemos R y P\n",
    "R = pca.components_\n",
    "P = pca.transform(X_s)\n",
    "print(R.shape)\n",
    "print(P.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz del dataset proyectado $\\mathbf{P}$ tiene varias propiedades importantes. Cada columna $\\mathbf{P}_{*,j}$ es el resultado de una combinación lineal de todas las columnas de $\\mathbf{X}$ multiplicadas por la columna de rotación correspondiente $\\mathbf{R}_{*,j}$ y las columnas en $\\mathbf{P}$ están descorrelacionadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pca.explained_variance_)\n",
    "pd.DataFrame(np.round(np.corrcoef(P.T), 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo más importante es que las varianzas de las columnas están ordenadas, de modo que la primera dimensión concentra la mayor parte de la varianza en el dataset, y así sucesivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.diag(np.cov(P.T)))\n",
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De modo que, si reducimos nuestros dataset a $p=2$ dimensiones, conservaríamos el 84.6% de la varianza del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pca.explained_variance_ratio_[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intentemos graficar la reducción a dos dimensiones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "colors=['red' if x=='Adelie' else 'green' if x=='Chinstrap' else 'black' for x in lab]\n",
    "plt.scatter(P[:,0], P[:,1], c=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos no-lineales\n",
    "\n",
    "PCA es lineal, en el sentido en que la transformación de $\\mathbf{X}$ a $\\mathbf{P}$ es una función lineal. Existe una gran cantidad de métodos no lineales para reducción de dimensionalidad, con distintos propósitos.\n",
    "\n",
    "Uno de los más útiles es t-SNE (t-Stochastic Nearest Neighbors Embedding), que es muy útil para visualización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "T = TSNE(n_components=2).fit_transform(X_s)\n",
    "plt.scatter(T[:,0], T[:,1], c=colors)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
