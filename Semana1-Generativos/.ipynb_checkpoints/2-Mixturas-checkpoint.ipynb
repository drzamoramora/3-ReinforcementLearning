{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de mixturas\n",
    "\n",
    "Veamos el siguiente conjunto de datos, con mediciones de peso de distintos individuos de ambos sexos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "peso = [52, 46, 52, 47, 47, 49, 50, 47, 53, 55, 44, 52, 47, 48, 51, 54, 51, 51, 53, 52, 47, 52, 47, 50, 47, 53, 52, 44, 54, 52, 55, 55, 53, 42, 53, 50, 49, 49, 46, 52, 56, 48, 50, 56, 49, 44, 53, 46, 50, 51, 42, 37, 43, 53, 57, 50, 51, 51, 46, 48, 50, 47, 51, 47, 49, 49, 47, 44, 53, 51, 56, 58, 47, 44, 55, 53, 46, 53, 52, 47, 49, 52, 36, 49, 55, 49, 55, 55, 42, 50, 49, 37, 53, 45, 52, 56, 51, 55, 48, 58, 49, 56, 54, 48, 56, 49, 48, 57, 55, 58, 52, 51, 39, 43, 47, 45, 50, 54, 50, 49, 56, 43, 44, 50, 47, 36, 53, 47, 43, 51, 50, 50, 52, 54, 46, 49, 52, 52, 56, 52, 54, 43, 51, 52, 56, 48, 49, 55, 51, 64, 55, 46, 45, 52, 54, 46, 47, 56, 58, 50, 48, 46, 42, 54, 50, 53, 42, 55, 45, 54, 50, 51, 56, 48, 49, 47, 47, 52, 43, 47, 48, 47, 46, 53, 53, 51, 49, 47, 50, 57, 55, 51, 48, 55, 45, 49, 47, 47, 49, 54, 46, 49, 48, 50, 47, 48, 50, 52, 53, 51, 54, 34, 49, 53, 63, 49, 47, 50, 56, 51, 54, 46, 45, 50, 58, 43, 52, 49, 53, 50, 56, 48, 54, 45, 54, 46, 43, 51, 40, 57, 51, 45, 53, 50, 53, 52, 48, 56, 60, 48, 50, 51, 46, 47, 53, 52, 45, 47, 54, 49, 52, 53, 60, 58, 56, 53, 54, 44, 51, 54, 53, 51, 45, 53, 52, 52, 51, 42, 55, 54, 53, 45, 52, 49, 53, 58, 44, 47, 47, 45, 55, 53, 45, 53, 48, 50, 58, 52, 50, 45, 49, 55, 55, 54, 42, 47, 41, 46, 50, 55, 52, 47, 49, 48, 52, 51, 53, 50, 48, 45, 51, 53, 51, 61, 51, 41, 55, 43, 56, 53, 47, 53, 51, 51, 52, 56, 42, 42, 47, 48, 56, 46, 43, 43, 48, 50, 50, 50, 49, 51, 51, 47, 41, 49, 47, 44, 50, 52, 49, 49, 49, 45, 54, 54, 54, 45, 47, 49, 52, 51, 45, 49, 47, 54, 53, 55, 46, 53, 50, 51, 51, 53, 49, 52, 55, 47, 53, 51, 53, 49, 58, 53, 58, 53, 51, 49, 51, 42, 51, 46, 45, 61, 50, 40, 44, 40, 58, 46, 47, 50, 51, 45, 54, 45, 58, 43, 51, 48, 48, 57, 48, 50, 61, 47, 53, 56, 49, 40, 46, 53, 49, 50, 51, 58, 56, 61, 46, 55, 58, 49, 49, 49, 45, 50, 47, 53, 46, 53, 45, 52, 53, 47, 54, 54, 52, 55, 50, 45, 53, 42, 52, 52, 52, 52, 51, 49, 44, 61, 49, 47, 56, 48, 56, 41, 47, 50, 57, 57, 47, 53, 47, 51, 50, 49, 50, 47, 55, 50, 54, 60, 42, 53, 46, 58, 52, 54, 53, 46, 46, 55, 52, 50, 43, 48, 54, 44, 43, 45, 53, 58, 49, 48, 51, 48, 48, 39, 55, 56, 58, 52, 50, 49, 49, 54, 49, 49, 48, 55, 52, 47, 54, 48, 53, 50, 51, 49, 59, 47, 64, 51, 53, 54, 52, 48, 54, 56, 46, 52, 48, 41, 51, 55, 56, 49, 52, 48, 45, 50, 56, 49, 46, 51, 55, 43, 53, 47, 56, 64, 49, 46, 56, 45, 53, 50, 53, 50, 51, 48, 49, 53, 41, 44, 50, 50, 50, 49, 51, 53, 48, 51, 50, 50, 46, 59, 50, 46, 47, 44, 45, 48, 44, 49, 52, 52, 50, 51, 51, 49, 52, 49, 56, 51, 56, 50, 46, 43, 53, 61, 53, 44, 52, 57, 51, 56, 50, 53, 54, 46, 57, 52, 64, 52, 55, 54, 52, 52, 49, 53, 51, 57, 48, 50, 50, 52, 41, 51, 52, 50, 54, 51, 47, 51, 40, 54, 54, 52, 51, 54, 51, 46, 48, 53, 48, 49, 46, 47, 61, 49, 46, 39, 51, 48, 49, 53, 42, 52, 50, 55, 49, 46, 46, 51, 51, 56, 51, 52, 52, 54, 44, 46, 42, 36, 48, 50, 50, 50, 43, 50, 47, 55, 49, 50, 47, 55, 49, 53, 37, 50, 46, 53, 47, 52, 56, 46, 58, 52, 51, 51, 55, 51, 46, 50, 54, 53, 53, 46, 48, 49, 53, 51, 38, 48, 51, 49, 47, 54, 49, 52, 51, 43, 47, 48, 48, 48, 43, 53, 64, 56, 46, 55, 55, 47, 47, 58, 52, 51, 44, 47, 52, 61, 51, 54, 61, 50, 45, 43, 51, 50, 48, 52, 53, 51, 48, 50, 48, 47, 52, 51, 50, 47, 51, 58, 58, 62, 53, 46, 51, 56, 44, 49, 54, 55, 47, 54, 44, 54, 51, 46, 48, 53, 54, 50, 55, 48, 50, 53, 50, 54, 47, 46, 44, 52, 50, 53, 43, 46, 51, 48, 49, 48, 45, 47, 41, 55, 52, 48, 53, 45, 62, 45, 47, 48, 52, 53, 53, 56, 38, 44, 48, 49, 47, 46, 52, 50, 49, 50, 47, 49, 55, 53, 46, 48, 47, 55, 52, 48, 47, 53, 53, 43, 46, 56, 48, 51, 40, 51, 57, 50, 57, 51, 53, 42, 45, 52, 51, 44, 50, 50, 53, 46, 47, 54, 50, 50, 56, 56, 53, 51, 49, 52, 46, 49, 47, 47, 50, 50, 41, 49, 52, 42, 47, 54, 47, 47, 51, 56, 47, 49, 51, 47, 48, 56, 44, 60, 41, 50, 52, 44, 48, 53, 48, 54, 54, 52, 44, 50, 46, 55, 52, 50, 51, 44, 56, 49, 50, 53, 52, 54, 52, 51, 54, 45, 48, 47, 45, 53, 45, 52, 52, 49, 52, 49, 45, 51, 44, 51, 49, 54, 47, 49, 53, 49, 46, 45, 52, 47, 47, 47, 43, 48, 51, 53, 54, 56, 46, 50, 55, 59, 54, 44, 48, 59, 47, 41, 48, 50, 45, 53, 53, 48, 49, 50, 53, 43, 59, 47, 44, 47, 51, 50, 56, 57, 55, 49, 56, 56, 54, 42, 49, 56, 51, 58, 44, 47, 56, 45, 53, 54, 47, 45, 50, 48, 47, 57, 53, 55, 47, 53, 49, 50, 46, 50, 48, 51, 46, 44, 51, 56, 55, 51, 47, 50, 49, 49, 47, 46, 51, 47, 51, 50, 48, 43, 49, 46, 46, 56, 49, 51, 51, 54, 50, 52, 59, 53, 46, 47, 50, 46, 47, 46, 49, 48, 49, 42, 47, 52, 48, 61, 49, 50, 54, 49, 49, 55, 52, 47, 51, 42, 55, 41, 54, 55, 50, 51, 51, 50, 53, 52, 46, 54, 52, 51, 56, 51, 53, 53, 44, 44, 53, 56, 54, 50, 46, 49, 50, 59, 48, 50, 54, 48, 46, 51, 44, 47, 54, 51, 55, 54, 51, 39, 52, 52, 49, 44, 47, 60, 51, 34, 46, 45, 52, 55, 57, 55, 42, 50, 57, 52, 53, 52, 45, 44, 42, 47, 46, 55, 47, 51, 54, 51, 47, 50, 57, 46, 49, 47, 47, 44, 44, 55, 52, 47, 48, 41, 48, 53, 43, 51, 45, 51, 55, 49, 54, 48, 52, 47, 47, 55, 43, 54, 48, 52, 51, 49, 48, 46, 49, 50, 47, 50, 49, 60, 51, 52, 53, 48, 51, 50, 42, 51, 52, 50, 52, 47, 51, 54, 47, 44, 53, 48, 45, 54, 49, 47, 46, 51, 52, 49, 47, 50, 48, 59, 54, 45, 61, 53, 43, 52, 54, 53, 61, 54, 49, 51, 50, 57, 43, 44, 51, 52, 47, 46, 48, 48, 51, 56, 43, 56, 46, 50, 53, 48, 45, 43, 50, 56, 48, 52, 48, 46, 48, 47, 42, 52, 49, 51, 49, 54, 48, 47, 54, 53, 42, 45, 46, 52, 55, 50, 42, 52, 45, 44, 44, 54, 48, 46, 48, 51, 55, 52, 50, 50, 48, 56, 52, 41, 56, 55, 50, 42, 53, 46, 47, 50, 46, 53, 56, 48, 50, 52, 53, 46, 57, 49, 51, 52, 50, 53, 53, 47, 47, 46, 55, 54, 48, 49, 49, 49, 51, 45, 45, 50, 48, 55, 52, 48, 48, 41, 50, 50, 50, 50, 61, 43, 52, 48, 53, 54, 49, 44, 47, 55, 58, 48, 58, 46, 44, 54, 55, 47, 47, 45, 47, 52, 50, 46, 52, 49, 48, 47, 50, 56, 50, 56, 54, 49, 54, 50, 50, 45, 56, 48, 56, 60, 50, 47, 54, 51, 53, 50, 50, 57, 56, 41, 53, 49, 54, 51, 57, 49, 50, 52, 47, 49, 53, 55, 49, 53, 47, 51, 50, 37, 61, 56, 50, 49, 67, 49, 52, 57, 55, 46, 53, 51, 57, 50, 50, 49, 49, 48, 56, 60, 42, 48, 49, 52, 56, 47, 57, 40, 60, 50, 52, 51, 54, 51, 47, 48, 59, 47, 53, 52, 49, 49, 49, 50, 39, 52, 48, 42, 50, 49, 50, 46, 52, 48, 51, 51, 51, 46, 49, 46, 48, 45, 55, 47, 47, 60, 57, 50, 51, 46, 49, 48, 47, 38, 52, 55, 51, 44, 48, 54, 51, 59, 51, 56, 46, 48, 51, 45, 56, 47, 51, 53, 50, 45, 53, 59, 50, 56, 53, 48, 42, 50, 54, 49, 50, 60, 50, 65, 58, 54, 47, 45, 55, 44, 51, 48, 49, 45, 46, 51, 48, 55, 53, 50, 46, 58, 59, 50, 49, 51, 54, 53, 61, 55, 52, 51, 45, 41, 52, 48, 50, 51, 52, 49, 51, 50, 50, 55, 44, 50, 47, 49, 55, 52, 58, 51, 50, 48, 45, 46, 55, 56, 43, 49, 50, 59, 53, 45, 47, 49, 40, 45, 55, 46, 50, 53, 50, 45, 52, 48, 53, 53, 51, 49, 52, 49, 52, 56, 36, 50, 45, 45, 49, 49, 56, 51, 50, 50, 48, 46, 50, 48, 48, 47, 52, 39, 44, 51, 55, 48, 55, 46, 53, 45, 54, 55, 57, 53, 49, 46, 53, 49, 49, 53, 52, 52, 51, 52, 51, 48, 53, 44, 39, 46, 48, 51, 52, 44, 54, 44, 49, 54, 55, 48, 51, 50, 54, 43, 53, 47, 47, 45, 54, 46, 52, 51, 55, 59, 48, 49, 55, 54, 51, 51, 44, 56, 50, 54, 47, 53, 58, 51, 54, 47, 43, 47, 55, 49, 75, 72, 59, 73, 74, 54, 62, 64, 68, 66, 82, 77, 79, 76, 63, 83, 76, 72, 75, 71, 66, 69, 67, 65, 67, 67, 69, 57, 52, 61, 72, 64, 74, 59, 72, 70, 68, 80, 70, 70, 72, 74, 78, 73, 76, 65, 68, 70, 75, 73, 68, 81, 76, 93, 73, 69, 66, 65, 61, 74, 74, 69, 76, 79, 73, 73, 82, 61, 62, 71, 83, 76, 61, 66, 77, 64, 72, 67, 68, 66, 67, 53, 79, 75, 61, 66, 71, 67, 71, 60, 72, 72, 72, 71, 69, 71, 78, 61, 71, 80, 69, 59, 64, 72, 75, 74, 69, 69, 70, 62, 61, 66, 77, 66, 67, 62, 64, 77, 69, 73, 78, 79, 63, 70, 72, 70, 70, 68, 70, 77, 88, 69, 62, 71, 74, 71, 58, 78, 68, 70, 74, 81, 68, 64, 67, 74, 64, 66, 65, 67, 65, 78, 78, 70, 64, 68, 72, 81, 68, 68, 64, 69, 64, 66, 65, 70, 80, 71, 59, 70, 69, 64, 78, 68, 71, 69, 69, 75, 62, 65, 59, 77, 62, 70, 72, 76, 84, 78, 60, 69, 74, 76, 76, 73, 73, 86, 65, 72, 72, 64, 82, 81, 73, 62, 65, 68, 74, 65, 73, 66, 80, 64, 65, 69, 65, 74, 74, 71, 65, 63, 66, 72, 72, 77, 77, 68, 75, 68, 72, 62, 68, 59, 68, 59, 73, 71, 82, 73, 71, 74, 73, 73, 71, 71, 75, 81, 72, 76, 75, 75, 84, 68, 65, 76, 71, 64, 67, 65, 65, 76, 80, 66, 80, 63, 72, 61, 76, 73, 61, 70, 72, 75, 71, 53, 73, 84, 74, 67, 68, 64, 70, 74, 68, 75, 63, 71, 73, 63, 72, 61, 68, 67, 62, 79, 65, 72, 72, 66, 69, 71, 68, 65, 81, 69, 70, 70, 61, 77, 73, 80, 73, 74, 60, 68, 71, 69, 81, 87, 66, 66, 71, 67, 65, 77, 67, 75, 66, 74, 77, 62, 72, 71, 80, 73, 64, 75, 69, 62, 67, 71, 70, 68, 64, 61, 80, 68, 83, 62, 70, 66, 75, 70, 61, 71, 64, 56, 55, 75, 78, 73, 62, 71, 67, 73, 65, 71, 67, 69, 66, 71, 69, 72, 77, 70, 68, 74, 70, 69, 68, 71, 80, 63, 70, 64, 70, 70, 68, 71, 65, 67, 63, 64, 73, 71, 62, 85, 73, 75, 73, 78, 69, 69, 77, 69, 70, 66, 61, 70, 71, 72, 68, 64, 60, 58, 68, 67, 71, 82, 60, 65, 73, 70, 74, 71, 69, 81, 60, 76, 70, 62, 67, 66, 67, 63, 72, 71, 62, 63, 65, 70, 66, 68, 79, 76, 79, 70, 68, 62, 66, 66, 75, 77, 68, 67, 73, 80, 58, 64, 78, 69, 65, 70, 70, 70, 75, 64, 59, 66, 74, 66, 84, 57, 79, 80, 63, 83, 84, 77, 74, 64, 67, 79, 64, 73, 70, 74, 75, 63, 68, 63, 70, 70, 66, 77, 70, 71, 79, 68, 70, 69, 66, 69, 69, 59, 73, 63, 73, 70, 63, 80, 78, 70, 77, 71, 68, 66, 78, 72, 71, 63, 77, 68, 71, 66, 73, 80, 77, 73, 70, 63, 61, 71, 67, 62, 80, 79, 62, 72, 61, 66, 69, 72, 75, 69, 76, 80, 76, 80, 76, 66, 77, 62, 72, 75, 62, 62, 61, 74, 72, 64, 70, 63, 69, 72, 73, 78, 63, 73, 63, 65, 69, 66, 72, 70, 85, 67, 63, 69, 72, 70, 70, 75, 76, 63, 66, 70, 67, 76, 74, 70, 70, 75, 67, 61, 61, 74, 77, 61, 68, 69, 78, 73, 76, 70, 63, 76, 62, 79, 73, 66, 79, 71, 64, 61, 69, 72, 72, 71, 75, 66, 77, 62, 63, 63, 64, 60, 69, 67, 72, 82, 75, 81, 76, 65, 66, 71, 82, 78, 59, 70, 72, 74, 73, 80, 75, 70, 78, 86, 77, 66, 68, 71, 68, 67, 78, 64, 71, 65, 64, 73, 73, 60, 72, 70, 76, 75, 68, 70, 60, 71, 57, 78, 74, 71, 70, 71, 59, 73, 76, 64, 76, 76, 70, 75, 65, 62, 70, 62, 64, 69, 68, 78, 81, 72, 67, 84, 67, 79, 65, 72, 62, 65, 71, 69, 61, 65, 58, 68, 68, 75, 60, 73, 63, 68, 87, 71, 69, 70, 61, 77, 75, 75, 72, 66, 75, 82, 71, 73, 61, 66, 81, 79, 71, 67, 66, 77, 64, 70, 70, 76, 69, 76, 74, 77, 71, 78, 70, 67, 64, 75, 69, 84, 64, 74, 65, 69, 73, 66, 77, 63, 74, 67, 61, 68, 59, 64, 59, 66, 65, 55, 66, 71, 75, 65, 58, 68, 75, 79, 69, 69, 67, 75, 71, 78, 75, 68, 54, 72, 75, 75, 62, 81, 67, 66, 78, 75, 59, 64, 74, 65, 67, 72, 67, 79, 66, 51, 72, 65, 64, 66, 61, 71, 67, 67, 65, 63, 68, 82, 62, 69, 87, 66, 72, 65, 58, 70, 83, 76, 70, 69, 75, 72, 63, 65, 66, 80, 72, 76, 68, 66, 78, 64, 73, 68, 69, 66, 80, 80, 68, 56, 67, 78, 59, 71, 70, 64, 81, 77, 78, 74, 61, 69, 70, 79, 61, 63, 71, 66, 74, 70, 69, 77, 76, 77, 75, 69, 77, 69, 68, 71, 77, 64, 65, 72, 69, 79, 68, 79, 68, 78, 73, 62, 58, 55, 76, 85, 75, 72, 77, 67, 68, 66, 83, 63, 88, 67, 77, 75, 80, 66, 86, 65, 77, 67, 63, 72, 75, 66, 68, 65, 71, 73, 72, 68, 80, 68, 81, 74, 73, 73, 77, 74, 68, 77, 70, 66, 72, 71, 77, 62, 65, 72, 78, 77, 69, 67, 78, 72, 66, 70, 64, 74, 75, 69, 66, 67, 72, 78, 65, 57, 74, 67, 61, 72, 66, 63, 66, 67, 72, 75, 65, 66, 75, 69, 62, 68, 61, 67, 67, 77, 70, 71, 65, 66, 66, 77, 68, 67, 74, 68, 70, 78, 60, 78, 69, 75, 73, 66, 76, 69, 66, 67, 65, 72, 74, 61, 65, 73, 77, 80, 75, 72, 66, 66, 70, 73, 77, 72, 67, 75, 71, 75, 68, 70, 77, 72, 64, 69, 66, 71, 61, 81, 74, 63, 65, 71, 77, 56, 81, 66, 64, 79, 72, 74, 64, 66, 74, 75, 71, 77, 74, 63, 86, 78, 85, 74, 68, 85, 63, 73, 61, 66, 68, 80, 69, 74, 65, 67, 57, 71, 70, 71, 68, 61, 62, 58, 65, 79, 77, 68, 64, 66, 72, 67, 74, 59, 73, 76, 69, 65, 74, 72, 66, 79, 79, 67, 72, 67, 68, 70, 60, 78, 78, 73, 74, 70, 50, 75, 68, 83, 71, 77, 66, 77, 80, 81, 69, 65, 70, 65, 63, 67, 65, 83, 69, 54, 74, 73, 71, 66, 70, 82, 54, 66, 69, 66, 62, 69, 67, 71, 61, 73, 72, 71, 65, 78, 70, 77, 70, 68, 66, 69, 71, 60, 68, 73, 69, 74, 67, 69, 73, 72, 73, 69, 65, 55, 74, 71, 67, 73, 62, 74, 75, 65, 70, 72, 69, 64, 70, 72, 73, 75, 70, 68, 66, 71, 81, 68, 80, 78, 72, 66, 65, 66, 64, 63, 74, 72, 67, 76, 76, 69, 61, 66, 75, 69, 66, 75, 58, 78, 67, 68, 73, 70, 77, 70, 80, 80, 73, 70, 66, 71, 72, 73, 63, 75, 66, 65, 87, 82, 75, 60, 65, 71, 67, 78, 77, 74, 65, 74, 68, 74, 76, 72, 70, 73, 62, 63, 71, 70, 70, 68, 73, 80, 78, 59, 69, 73, 63, 73, 61, 67, 81, 76, 69, 68, 58, 66, 63, 68, 59, 68, 65, 79, 72, 64, 68, 75, 67, 68, 67, 71, 70, 73, 72, 74, 67, 66, 74, 59, 72, 60, 72, 74, 81, 64, 72, 79, 67, 78]\n",
    "sexo = ['F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M']\n",
    "plt.hist([x for (x,y) in list(zip(peso,sexo)) if y=='M'], color='blue', alpha=0.5, bins=15)\n",
    "plt.hist([x for (x,y) in list(zip(peso,sexo)) if y=='F'], color='red', alpha=0.5, bins=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La distribución es **bimodal**. Una gaussiana convencional sería un modelo pobre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "mu = np.array(peso).mean()\n",
    "sig = np.array(peso).std()\n",
    "plt.hist(peso, color='gray', density=True, bins=30)\n",
    "x = np.linspace(mu - 3*sig, mu + 3*sig, 100)\n",
    "plt.plot(x, stats.norm.pdf(x, mu, sig), color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para atacar este tipo de problemas, introduciremos el concepto de **mixtura**, la cual es una combinación convexa de $K$ distribuciones de probabilidad $p_1(x), p_2(x), \\ldots, p_K(x)$, ponderadas por un conjunto de **pesos de mixtura** $\\pi_1, \\pi_2, ..., \\pi_K$ donde $\\sum_{i=i}^{K} \\pi_i = 1$. La función de densidad de mixtura es\n",
    "\n",
    "$$p(x) = \\sum_{i=1}^{K} \\pi_i p_i \\left(x \\right)$$\n",
    "\n",
    "Es posible demostrar que una mixtura **también es una distribución de probabilidad**.\n",
    "\n",
    "Volvamos al ejemplo anterior. Podemos modelarlo como una mixtura de dos gaussianas $\\mathcal{N}_1 \\left(\\mu_1, \\sigma_1 \\right)$ y $\\mathcal{N}_2 \\left(\\mu_2, \\sigma_2 \\right)$.\n",
    "\n",
    "Entonces, la función de distribución de nuestra mixtura es $$p(x) = \\pi_1\\mathcal{N}_1 \\left(x | \\mu_1, \\sigma_1 \\right) + \\pi_2\\mathcal{N}_2 \\left(x | \\mu_2, \\sigma_2 \\right)$$\n",
    "\n",
    "Nuestro **objetivo de aprendizaje** es encontrar los valores de $\\pi_1, \\pi_2, \\mu_1, \\sigma_1, \\mu_2, \\sigma_2$\n",
    "\n",
    "## El algoritmo EM\n",
    "\n",
    "En el modelo de una sola gaussiana podemos derivar fórmulas para estimar $\\mu$ y $\\sigma$. Con las mixturas no se puede (no están en forma cerrada). Así que hay que aplicar un método de aproximación numérica: EM (*Expectation-Maximization*)\n",
    "\n",
    "  - Inicializar parámetros de distribución $\\theta$\n",
    "  - Repetir hasta que haya convergencia (o hasta que se alcance un número máximo de iteraciones)\n",
    "    - Paso E: Calcular expectativas sobre cada punto de datos $x_i, i \\in \\lbrace1,\\ldots,N \\rbrace$ usando los parámetros de distribución $\\theta$\n",
    "    - Paso M: Calcular parámetros $\\theta$ de distribución usando las expectativas\n",
    "    \n",
    "En el caso de nuestro modelo, la **expectativa** de cada punto $x_i$ es un par $\\left(\\pi_{i,1}, \\pi_{i,2} \\right)$ que representan la **probabilidad de** $x_i$ bajo la distribución $\\mathcal{N}_1$ y a $\\mathcal{N}_2$, respectivamente. Como $K=2$, sabemos que $\\pi_{i,2} = 1 - \\pi_{i,1}$. Esta expectativa se calcula como \n",
    "\n",
    "$$\\pi_{i,1} = \\frac{\\hat{\\pi}_{i,1}\\mathcal{N}\\left(x_i | \\mu_1, \\sigma_1 \\right)}{\\hat{\\pi}_{i,1}\\mathcal{N}\\left(x_i | \\mu_1, \\sigma_1 \\right) + \\hat{\\pi}_{i,2}\\mathcal{N}\\left(x_i | \\mu_2, \\sigma_2 \\right)}$$ \n",
    "\n",
    "donde $\\hat{\\pi}$ se refiere a las expectativas calculadas en la iteración anterior.\n",
    "\n",
    "Luego, la **maximización** la obtenemos calculando los parámetros de ambas gaussianas **ponderando cada punto** $x_i$ **de acuerdo a su expectativa**, de modo que:\n",
    "\n",
    "$$\\mu_1 = \\frac{\\sum_{i=1}^{N} x_i \\pi_{i,1}}{\\sum_{i=1}^{N} \\pi_{i,1}} , \\textrm{idem con } \\mu_2$$\n",
    "\n",
    "$$\\sigma_1^2 = \\frac{\\sum_{i=1}^{N} \\pi_{i,1} \\left(x_i - \\mu_1 \\right)^2}{\\sum_{i=1}^{N} \\pi_{i,1}} , \\textrm{idem con } \\sigma_2$$\n",
    "\n",
    "\n",
    "¿Parece familiar este algoritmo? **K-Medias** es un caso específico de EM, con componentes gaussianas **isotrópicas** y una restricción de asignación total. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peso = np.array(peso)\n",
    "\n",
    "# inicialización de parámetros\n",
    "\n",
    "# medias\n",
    "[mu_1, mu_2] = peso.mean() + np.random.randn(2)*peso.std()\n",
    "\n",
    "# desviaciones estándar\n",
    "sig_1 = peso.std()\n",
    "sig_2 = peso.std()\n",
    "\n",
    "# pesos de cada punto de datos\n",
    "pi_1 = np.ones(peso.shape[0]) / 2\n",
    "pi_2 = 1-pi_1\n",
    "\n",
    "# iteración\n",
    "num_iter = 1000\n",
    "for iter in range(0, num_iter):\n",
    "    \n",
    "    # --> paso E    \n",
    "    \n",
    "    # probabidad de cada punto en la distribución 1\n",
    "    pp1 = pi_1 * stats.norm.pdf(peso, mu_1, sig_1)\n",
    "    \n",
    "    # probabilidad de cada punto en la distribución 2\n",
    "    pp2 = pi_2 * stats.norm.pdf(peso, mu_2, sig_2)   \n",
    "    \n",
    "    # normalizamos probabilidades\n",
    "    pi_1 = pp1 / (pp1 + pp2)\n",
    "    pi_2 = 1 - pi_1\n",
    "\n",
    "    # --> paso M\n",
    "    \n",
    "    # media de la distribución 1\n",
    "    new_mu_1 = (pi_1*peso).sum() / pi_1.sum()\n",
    "    \n",
    "    # media de la distribución 2\n",
    "    new_mu_2 = (pi_2*peso).sum() / pi_2.sum()\n",
    "    \n",
    "    # desviación estándar de la distribución 1 & 2   \n",
    "    sig_1 = np.sqrt((pi_1*(peso - mu_1)**2).sum() / pi_1.sum())\n",
    "    sig_2 = np.sqrt((pi_2*(peso - mu_2)**2).sum() / pi_2.sum())\n",
    "    \n",
    "    mu_1 = new_mu_1\n",
    "    mu_2 = new_mu_2\n",
    "    \n",
    "# dibujemos la distribución aprendida encima del histograma de peso\n",
    "plt.hist(peso, color='gray', density=True, bins=30)\n",
    "x = np.linspace(mu - 3*sig, mu + 3*sig, 100)\n",
    "y = pi_1.mean()*stats.norm.pdf(x, mu_1, sig_1) + pi_2.mean()*stats.norm.pdf(x, mu_2, sig_2)\n",
    "plt.plot(x, y, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixturas gaussianas en ``scikit-learn``\n",
    "\n",
    "La clase ``sklearn.mixture.GaussianMixture`` hace el trabajo por nosotros y es sumamente fácil de utilizar. El parámetro ``n_components`` define el número de distribuciones. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "peso_r = peso.reshape((peso.shape[0], 1))\n",
    "gmm = GaussianMixture(n_components=2)\n",
    "gmm.fit(peso_r)\n",
    "plt.hist(peso, color='gray', density=True, bins=30)\n",
    "\n",
    "# calculamos probabilidades -- vienen en escala logarítmica\n",
    "x = np.linspace(mu - 3*sig, mu + 3*sig, 100)\n",
    "x_r = x.reshape((x.shape[0], 1))\n",
    "yy = np.exp(gmm.score_samples(x_r))\n",
    "plt.plot(x, yy, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Mixturas gaussianas multivariadas\n",
    "\n",
    "Para nuestro ejemplo, hemos utilizado gaussianas unidimensionales. La distribución gaussiana multivariada es una herramienta esencial en ML y es la **generalización** de la gaussiana a **dos dimensiones** o más (p.ej. peso y estatura).\n",
    "\n",
    "Una distribución gaussiana de dimensión $d$ tiene los siguientes parámetros: \n",
    "\n",
    "  - **Vector de medias** $\\boldsymbol{\\mu}$ de longitud $d$.\n",
    "      - Elemento $\\boldsymbol{\\mu}_i$: media de la distribución correspondiente a la dimensión $i$.\n",
    "  - **Matriz de covarianza** $\\boldsymbol{\\Sigma}$ de dimensión $d \\times d$.\n",
    "      - Simétrica, positiva definida\n",
    "        - $\\boldsymbol{\\Sigma}_{ij} = \\boldsymbol{\\Sigma}_{ji}$\n",
    "        - $z^{T}\\Sigma$ positivo para todo vector $z > 0$\n",
    "      - Elemento $\\boldsymbol{\\Sigma}_{ij}$: covarianza entre las distribuciones correspondientes a dimensiones $i$ y $j$\n",
    "      - Diagonal $\\boldsymbol{\\Sigma}_{ii}$: varianza de cada dimensión\n",
    "      - Si $\\boldsymbol{\\Sigma}_{ij} = 0$, entonces $i$ y $j$ son independientes\n",
    "      - Si solo diagonal $\\boldsymbol{\\Sigma}_{ii} > 0$: \"isotrópica\"\n",
    "        - De lo contrario, \"rotada\"\n",
    "\n",
    "Veamos unos ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "x, y = np.mgrid[-4:4:.005, -4:4:.005]\n",
    "pos = np.dstack((x, y))\n",
    "\n",
    "# Estándar isotrópica\n",
    "mvn = multivariate_normal([0.0, 0.0], [[1.0, 0.0], [0.0, 1.0]])\n",
    "plt.contourf(x, y, mvn.pdf(pos))\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()\n",
    "\n",
    "# Isotrópica, pero elíptica\n",
    "mvn = multivariate_normal([0.0, 0.0], [[3.0, 0.0], [0.0, 1.0]])\n",
    "plt.contourf(x, y, mvn.pdf(pos))\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()\n",
    "\n",
    "# Con covarianza (rotada)\n",
    "mvn = multivariate_normal([0.0, 0.0], [[1.0, 0.5], [0.5, 1.0]])\n",
    "plt.contourf(x, y, mvn.pdf(pos))\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, procedamos a ver como sería un dataset **multimodal** en el que queremos utilizar una **mixtura** de gaussianas multivariadas. Introducimos la variable estatura y veamos los dos componentes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estatura = [158, 161, 163, 158, 157, 164, 159, 158, 163, 165, 151, 162, 162, 155, 158, 158, 159, 162, 164, 161, 155, 159, 159, 159, 157, 163, 163, 156, 161, 164, 165, 168, 157, 160, 162, 162, 159, 152, 157, 158, 162, 157, 158, 164, 161, 160, 161, 157, 161, 164, 153, 155, 157, 164, 165, 159, 159, 161, 157, 164, 160, 160, 166, 158, 163, 158, 161, 160, 160, 162, 163, 157, 157, 156, 168, 165, 161, 165, 154, 157, 162, 158, 153, 163, 166, 164, 158, 163, 153, 158, 159, 153, 159, 153, 157, 165, 164, 162, 159, 164, 158, 160, 162, 157, 161, 156, 162, 162, 158, 165, 163, 162, 159, 154, 161, 159, 160, 159, 163, 160, 157, 158, 154, 161, 157, 154, 158, 153, 155, 160, 159, 161, 157, 161, 160, 161, 156, 162, 160, 162, 162, 158, 163, 159, 163, 161, 161, 156, 157, 162, 161, 157, 159, 164, 170, 155, 160, 163, 162, 159, 160, 156, 156, 164, 158, 160, 156, 161, 156, 162, 158, 161, 166, 160, 161, 160, 156, 162, 154, 162, 154, 158, 154, 159, 162, 162, 159, 156, 161, 158, 163, 163, 161, 167, 157, 160, 160, 158, 157, 159, 156, 158, 158, 160, 162, 156, 161, 167, 161, 154, 165, 152, 158, 161, 164, 157, 155, 157, 165, 157, 156, 156, 158, 160, 163, 156, 165, 158, 162, 160, 156, 158, 153, 155, 166, 159, 157, 158, 154, 165, 158, 158, 166, 165, 165, 156, 160, 162, 165, 160, 159, 158, 160, 157, 162, 161, 154, 161, 164, 164, 162, 162, 163, 168, 166, 161, 164, 157, 159, 165, 160, 162, 159, 164, 162, 161, 157, 159, 161, 166, 160, 158, 163, 163, 165, 166, 156, 158, 160, 159, 163, 162, 154, 164, 153, 157, 161, 159, 163, 155, 162, 157, 162, 162, 155, 157, 155, 161, 158, 166, 161, 161, 163, 158, 162, 160, 161, 160, 158, 151, 160, 158, 163, 169, 158, 152, 163, 152, 159, 161, 161, 164, 159, 161, 159, 162, 154, 156, 159, 166, 159, 158, 159, 152, 156, 158, 162, 157, 162, 159, 159, 162, 150, 163, 159, 156, 163, 166, 162, 160, 161, 158, 158, 163, 159, 158, 158, 161, 156, 154, 160, 160, 158, 164, 158, 160, 160, 161, 159, 156, 159, 160, 158, 158, 163, 160, 164, 162, 163, 160, 166, 165, 166, 159, 162, 164, 161, 157, 163, 156, 160, 165, 160, 155, 156, 156, 159, 161, 152, 163, 161, 153, 158, 155, 159, 159, 163, 160, 164, 162, 154, 163, 163, 154, 165, 161, 163, 158, 163, 164, 158, 160, 161, 165, 161, 164, 158, 160, 168, 158, 159, 161, 161, 158, 164, 159, 157, 159, 161, 163, 161, 158, 164, 163, 161, 165, 161, 159, 157, 161, 162, 158, 159, 156, 161, 161, 159, 164, 160, 159, 163, 162, 166, 153, 160, 160, 162, 160, 157, 163, 154, 159, 156, 159, 162, 160, 160, 161, 161, 168, 151, 163, 160, 165, 167, 163, 164, 157, 152, 163, 162, 161, 156, 161, 160, 162, 156, 155, 162, 162, 164, 158, 159, 157, 159, 154, 163, 166, 161, 163, 163, 158, 159, 165, 160, 160, 159, 162, 160, 156, 162, 160, 160, 157, 161, 162, 162, 157, 168, 164, 167, 160, 157, 157, 159, 162, 154, 159, 160, 162, 160, 160, 165, 162, 161, 164, 158, 163, 164, 161, 160, 164, 159, 158, 164, 159, 165, 169, 159, 158, 160, 157, 162, 159, 156, 161, 162, 159, 165, 162, 158, 159, 160, 162, 153, 161, 163, 162, 160, 159, 162, 161, 162, 163, 159, 154, 158, 158, 162, 153, 154, 159, 161, 163, 161, 163, 161, 157, 155, 163, 163, 157, 165, 161, 153, 155, 162, 162, 160, 152, 159, 164, 154, 167, 160, 159, 158, 161, 163, 164, 163, 162, 157, 164, 158, 160, 155, 162, 165, 167, 160, 162, 157, 160, 157, 156, 159, 158, 161, 160, 160, 156, 156, 167, 163, 161, 158, 160, 162, 160, 162, 164, 156, 158, 156, 159, 162, 159, 167, 157, 157, 159, 161, 164, 153, 165, 161, 161, 159, 158, 157, 162, 169, 163, 155, 158, 157, 158, 160, 154, 155, 155, 160, 160, 159, 159, 157, 158, 154, 168, 159, 163, 155, 162, 161, 166, 150, 160, 158, 163, 156, 163, 156, 161, 165, 161, 159, 160, 162, 157, 156, 160, 163, 163, 158, 163, 159, 160, 167, 164, 154, 158, 159, 162, 161, 162, 159, 161, 162, 156, 159, 157, 162, 162, 160, 158, 167, 165, 163, 159, 165, 154, 159, 165, 166, 157, 157, 160, 161, 163, 161, 161, 169, 161, 159, 164, 164, 165, 157, 158, 161, 162, 160, 161, 159, 157, 165, 160, 161, 154, 163, 161, 163, 162, 161, 162, 159, 161, 156, 163, 162, 164, 156, 165, 159, 162, 158, 159, 158, 164, 164, 158, 163, 161, 156, 166, 160, 160, 157, 161, 157, 165, 161, 164, 156, 162, 163, 160, 159, 157, 156, 159, 160, 162, 157, 159, 163, 159, 165, 159, 156, 159, 156, 164, 161, 161, 151, 161, 158, 159, 159, 161, 162, 155, 157, 158, 158, 164, 159, 157, 161, 164, 157, 162, 164, 157, 153, 159, 162, 158, 157, 164, 154, 158, 153, 162, 165, 161, 159, 159, 157, 152, 153, 162, 161, 156, 156, 158, 157, 159, 159, 158, 167, 157, 165, 162, 159, 159, 155, 162, 157, 160, 159, 160, 160, 163, 153, 159, 159, 155, 157, 161, 162, 162, 161, 163, 163, 165, 154, 155, 161, 165, 158, 162, 158, 162, 156, 153, 155, 160, 159, 159, 161, 159, 156, 159, 167, 165, 163, 160, 161, 156, 160, 165, 158, 159, 161, 157, 160, 165, 160, 158, 158, 162, 160, 163, 160, 166, 169, 161, 164, 158, 159, 166, 153, 162, 160, 156, 162, 160, 162, 156, 155, 158, 163, 158, 158, 154, 160, 161, 162, 160, 159, 163, 161, 154, 166, 165, 166, 155, 156, 171, 160, 156, 156, 158, 157, 159, 162, 161, 160, 162, 158, 157, 166, 158, 156, 159, 159, 161, 163, 164, 163, 159, 165, 161, 161, 160, 156, 162, 159, 164, 159, 163, 163, 158, 156, 164, 161, 163, 158, 161, 158, 165, 163, 159, 159, 156, 156, 159, 160, 157, 159, 157, 160, 157, 162, 166, 166, 162, 158, 157, 164, 159, 164, 159, 165, 159, 163, 167, 158, 155, 160, 163, 158, 157, 157, 155, 159, 163, 161, 157, 167, 166, 156, 159, 162, 163, 157, 155, 161, 155, 161, 157, 157, 164, 159, 162, 159, 164, 168, 160, 160, 157, 162, 157, 162, 161, 162, 158, 166, 159, 160, 158, 162, 158, 163, 160, 160, 157, 161, 157, 172, 161, 158, 164, 157, 162, 161, 162, 161, 162, 158, 160, 156, 162, 159, 162, 161, 160, 162, 160, 158, 157, 164, 155, 160, 162, 158, 154, 166, 160, 161, 156, 160, 168, 161, 153, 157, 156, 164, 168, 164, 163, 155, 163, 168, 166, 160, 159, 163, 157, 155, 160, 161, 164, 161, 155, 160, 162, 161, 158, 169, 158, 160, 161, 159, 163, 159, 161, 160, 159, 160, 155, 159, 160, 154, 157, 156, 156, 167, 160, 160, 159, 163, 157, 159, 161, 153, 164, 162, 162, 161, 158, 160, 159, 162, 165, 159, 158, 161, 168, 162, 160, 160, 162, 159, 155, 156, 159, 163, 159, 160, 162, 162, 161, 159, 154, 158, 162, 159, 160, 163, 160, 159, 159, 161, 161, 160, 162, 158, 161, 162, 161, 167, 163, 155, 168, 162, 165, 165, 162, 160, 161, 156, 166, 159, 160, 158, 160, 159, 160, 159, 156, 160, 166, 159, 162, 164, 157, 163, 161, 153, 157, 158, 163, 163, 161, 161, 157, 157, 160, 152, 157, 161, 161, 158, 162, 162, 153, 163, 169, 157, 158, 155, 161, 160, 157, 157, 162, 159, 159, 157, 163, 161, 161, 155, 163, 164, 164, 165, 163, 158, 162, 160, 153, 160, 165, 163, 152, 164, 161, 164, 158, 162, 159, 161, 164, 162, 163, 162, 154, 165, 161, 163, 164, 160, 157, 160, 157, 159, 152, 161, 163, 164, 158, 161, 165, 157, 158, 159, 166, 159, 165, 165, 161, 159, 149, 160, 158, 159, 159, 167, 159, 163, 157, 161, 161, 157, 157, 158, 160, 163, 158, 169, 159, 159, 159, 161, 154, 157, 159, 156, 162, 162, 163, 160, 155, 160, 152, 163, 165, 157, 162, 165, 161, 168, 161, 158, 154, 161, 157, 159, 161, 158, 158, 161, 163, 161, 162, 165, 165, 167, 158, 158, 161, 159, 161, 165, 159, 164, 159, 156, 161, 162, 159, 162, 160, 158, 158, 159, 155, 169, 158, 161, 158, 169, 157, 155, 166, 163, 154, 161, 161, 162, 160, 160, 158, 162, 161, 170, 165, 152, 160, 157, 164, 159, 159, 167, 159, 163, 161, 163, 167, 160, 158, 163, 161, 169, 155, 165, 161, 162, 159, 156, 164, 155, 158, 161, 158, 163, 165, 158, 155, 160, 159, 165, 160, 160, 163, 161, 156, 158, 159, 165, 160, 158, 166, 164, 164, 164, 158, 166, 164, 157, 152, 162, 156, 163, 161, 161, 162, 163, 162, 162, 164, 156, 158, 168, 157, 163, 158, 159, 165, 163, 152, 161, 165, 161, 163, 157, 161, 154, 159, 158, 161, 159, 164, 159, 166, 163, 165, 160, 157, 159, 156, 160, 160, 160, 158, 164, 161, 160, 161, 160, 160, 154, 166, 163, 160, 162, 162, 157, 162, 163, 166, 157, 160, 163, 156, 159, 160, 159, 158, 161, 159, 157, 162, 163, 167, 155, 160, 155, 158, 163, 162, 162, 159, 158, 164, 155, 161, 164, 164, 158, 155, 149, 169, 165, 158, 153, 163, 152, 153, 167, 159, 156, 163, 162, 159, 162, 159, 159, 161, 162, 163, 163, 155, 161, 167, 158, 158, 159, 157, 165, 158, 166, 162, 161, 164, 159, 161, 164, 155, 158, 156, 163, 149, 155, 157, 161, 157, 164, 161, 162, 156, 161, 160, 160, 163, 161, 154, 159, 162, 153, 164, 165, 168, 160, 162, 161, 156, 163, 156, 156, 164, 161, 163, 161, 159, 162, 156, 157, 159, 162, 160, 165, 157, 164, 156, 161, 153, 161, 158, 159, 156, 162, 159, 167, 161, 162, 159, 160, 167, 163, 161, 157, 167, 162, 169, 158, 161, 160, 166, 159, 159, 157, 159, 166, 164, 171, 178, 168, 169, 172, 161, 166, 171, 173, 172, 181, 171, 180, 169, 166, 178, 180, 173, 173, 174, 171, 164, 166, 163, 173, 169, 164, 163, 163, 162, 171, 171, 176, 162, 169, 171, 177, 178, 176, 174, 171, 174, 173, 172, 168, 163, 172, 180, 172, 166, 174, 172, 176, 179, 168, 173, 169, 166, 162, 178, 168, 175, 178, 178, 168, 169, 186, 161, 160, 171, 180, 174, 160, 171, 176, 164, 166, 166, 177, 172, 171, 161, 171, 166, 164, 168, 167, 163, 168, 160, 167, 166, 166, 170, 167, 168, 178, 164, 171, 175, 168, 162, 167, 174, 171, 173, 172, 171, 168, 164, 164, 165, 175, 161, 169, 162, 173, 170, 168, 167, 181, 175, 168, 169, 170, 169, 166, 166, 170, 175, 181, 172, 158, 172, 178, 168, 170, 174, 166, 170, 172, 173, 173, 170, 163, 173, 159, 167, 166, 164, 166, 172, 172, 168, 165, 168, 172, 178, 162, 167, 168, 169, 168, 162, 168, 164, 175, 169, 164, 170, 171, 170, 176, 165, 169, 171, 174, 169, 167, 166, 163, 178, 167, 173, 171, 176, 179, 177, 165, 169, 176, 172, 172, 172, 176, 181, 162, 169, 172, 163, 176, 177, 172, 170, 171, 172, 172, 167, 165, 170, 183, 168, 167, 167, 172, 169, 173, 174, 171, 161, 169, 173, 170, 178, 175, 171, 175, 169, 173, 165, 168, 162, 170, 161, 173, 172, 178, 173, 175, 164, 173, 170, 174, 174, 173, 181, 172, 173, 172, 174, 183, 166, 167, 173, 171, 169, 163, 167, 165, 175, 179, 165, 176, 163, 172, 163, 171, 170, 161, 168, 170, 178, 168, 161, 173, 180, 175, 165, 166, 172, 172, 172, 171, 172, 168, 172, 179, 163, 172, 163, 167, 171, 160, 174, 166, 168, 173, 169, 168, 173, 166, 169, 178, 168, 170, 170, 166, 178, 172, 172, 169, 176, 164, 170, 174, 170, 182, 185, 175, 170, 171, 169, 169, 179, 167, 171, 167, 173, 171, 169, 172, 170, 175, 172, 165, 174, 169, 162, 172, 170, 171, 173, 170, 163, 178, 171, 173, 164, 168, 168, 178, 170, 162, 175, 166, 158, 162, 173, 176, 174, 173, 167, 168, 175, 164, 170, 168, 173, 167, 173, 165, 170, 175, 169, 168, 171, 168, 168, 167, 172, 174, 169, 171, 165, 170, 170, 166, 171, 165, 168, 161, 166, 173, 168, 164, 175, 170, 169, 174, 174, 165, 175, 179, 160, 168, 167, 159, 170, 173, 168, 167, 168, 165, 164, 165, 164, 172, 176, 161, 170, 168, 172, 166, 172, 170, 184, 163, 176, 172, 163, 170, 170, 164, 165, 172, 171, 161, 158, 170, 172, 167, 167, 179, 181, 175, 167, 168, 166, 167, 171, 175, 173, 167, 165, 174, 181, 164, 169, 171, 175, 168, 168, 169, 169, 177, 167, 166, 167, 174, 166, 179, 167, 178, 176, 174, 180, 182, 172, 172, 170, 171, 176, 164, 174, 174, 169, 170, 167, 161, 167, 171, 174, 170, 175, 166, 166, 178, 170, 171, 171, 167, 166, 172, 164, 172, 165, 172, 171, 158, 174, 173, 176, 171, 172, 175, 163, 176, 175, 172, 168, 170, 168, 171, 168, 169, 177, 175, 172, 167, 166, 166, 172, 173, 167, 172, 181, 171, 167, 164, 166, 164, 173, 170, 168, 174, 183, 175, 181, 171, 162, 177, 168, 173, 174, 166, 168, 160, 175, 173, 168, 171, 164, 172, 167, 168, 172, 167, 173, 166, 167, 172, 163, 174, 167, 183, 170, 169, 168, 170, 173, 172, 167, 175, 164, 168, 162, 172, 175, 178, 173, 169, 176, 169, 167, 169, 175, 177, 163, 171, 167, 171, 176, 172, 172, 167, 177, 166, 175, 168, 168, 178, 171, 167, 163, 168, 172, 168, 169, 170, 160, 174, 164, 169, 167, 163, 166, 168, 169, 174, 186, 174, 177, 174, 165, 164, 171, 180, 173, 161, 170, 173, 170, 174, 174, 175, 170, 176, 182, 168, 168, 170, 174, 167, 170, 169, 167, 174, 170, 164, 170, 167, 161, 175, 174, 174, 174, 168, 170, 161, 171, 168, 169, 171, 170, 172, 169, 162, 171, 172, 166, 174, 174, 172, 174, 170, 164, 169, 170, 168, 175, 172, 170, 176, 175, 169, 184, 171, 176, 168, 171, 163, 164, 168, 167, 170, 166, 165, 165, 172, 178, 167, 177, 171, 169, 182, 176, 172, 171, 167, 174, 172, 170, 170, 163, 175, 175, 174, 173, 164, 165, 178, 176, 173, 171, 166, 175, 165, 173, 162, 167, 174, 175, 163, 176, 174, 175, 167, 171, 163, 176, 166, 177, 159, 175, 167, 169, 173, 166, 174, 168, 172, 169, 159, 169, 173, 163, 164, 167, 164, 161, 165, 161, 174, 170, 158, 170, 176, 178, 174, 165, 171, 177, 172, 172, 176, 168, 155, 168, 175, 174, 166, 177, 166, 168, 171, 170, 167, 170, 176, 168, 168, 176, 166, 175, 165, 158, 172, 170, 165, 163, 165, 168, 163, 170, 167, 165, 172, 176, 161, 172, 175, 164, 169, 167, 166, 170, 178, 175, 169, 171, 171, 179, 167, 164, 166, 176, 171, 171, 168, 170, 175, 165, 166, 171, 173, 168, 176, 180, 167, 167, 164, 172, 162, 169, 170, 165, 173, 175, 178, 171, 161, 166, 167, 175, 163, 165, 169, 165, 168, 173, 164, 170, 176, 176, 173, 169, 176, 168, 168, 168, 178, 163, 168, 172, 166, 178, 169, 174, 162, 170, 174, 174, 164, 159, 170, 179, 168, 168, 168, 172, 164, 163, 177, 168, 180, 170, 178, 173, 180, 170, 181, 165, 173, 168, 162, 178, 173, 171, 162, 165, 167, 170, 170, 172, 171, 165, 175, 172, 179, 172, 173, 168, 170, 173, 171, 169, 168, 164, 176, 161, 172, 171, 172, 175, 170, 172, 175, 172, 162, 172, 167, 175, 173, 167, 167, 168, 179, 176, 166, 157, 171, 168, 165, 171, 167, 166, 172, 167, 168, 174, 165, 166, 172, 167, 164, 167, 163, 168, 166, 175, 171, 168, 168, 169, 169, 175, 169, 167, 175, 172, 167, 176, 163, 177, 169, 177, 176, 165, 167, 168, 166, 168, 168, 174, 174, 165, 165, 169, 173, 173, 172, 173, 166, 165, 164, 170, 175, 171, 165, 176, 169, 169, 166, 169, 176, 169, 160, 171, 172, 168, 159, 177, 170, 165, 159, 165, 172, 158, 180, 173, 167, 182, 174, 174, 164, 171, 175, 175, 170, 177, 177, 163, 181, 172, 180, 169, 166, 185, 166, 178, 157, 168, 167, 177, 168, 172, 167, 171, 163, 170, 167, 168, 166, 162, 166, 162, 161, 172, 171, 167, 167, 170, 175, 170, 176, 168, 170, 172, 166, 168, 177, 171, 169, 175, 177, 166, 171, 168, 168, 169, 166, 176, 172, 172, 170, 163, 153, 179, 166, 180, 166, 170, 169, 176, 183, 176, 169, 167, 177, 168, 164, 169, 173, 176, 170, 160, 173, 165, 173, 168, 177, 177, 162, 170, 169, 166, 169, 175, 167, 172, 165, 170, 174, 173, 169, 172, 168, 177, 171, 172, 175, 172, 169, 163, 165, 163, 171, 168, 171, 170, 169, 170, 169, 166, 166, 157, 166, 165, 172, 169, 169, 173, 168, 159, 166, 171, 171, 164, 170, 169, 174, 173, 167, 165, 167, 168, 175, 168, 172, 173, 171, 169, 171, 171, 167, 167, 169, 171, 172, 173, 172, 172, 166, 165, 176, 170, 174, 183, 159, 175, 163, 171, 168, 165, 176, 167, 176, 177, 173, 174, 168, 176, 166, 171, 163, 172, 167, 171, 179, 172, 174, 166, 167, 174, 163, 176, 173, 169, 167, 173, 167, 172, 172, 171, 169, 174, 166, 168, 172, 165, 170, 170, 176, 175, 172, 160, 169, 174, 167, 176, 165, 166, 174, 169, 171, 167, 163, 169, 166, 165, 163, 166, 169, 176, 172, 170, 171, 171, 169, 170, 173, 171, 170, 169, 172, 176, 168, 168, 176, 161, 172, 164, 171, 175, 178, 167, 172, 175, 163, 181]\n",
    "\n",
    "# dibujemos un plot 2D y un histograma para mostrar la densidad\n",
    "plt.scatter(peso, estatura, color='gray')\n",
    "plt.show()\n",
    "plt.hist2d(peso, estatura, bins=(60,30), cmap=\"hot\")\n",
    "plt.show()\n",
    "plt.scatter(peso, estatura, color='gray', alpha=0.25)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando ``sklearn`` podemos trabajar con cualquier dimensionalidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# ponemos los datos juntos\n",
    "X = np.stack((peso, estatura)).T\n",
    "gmm = GaussianMixture(n_components=2)\n",
    "gmm.fit(X)\n",
    "\n",
    "vx, vy = np.mgrid[X[:,0].min():X[:,0].max():0.5, X[:,1].min():X[:,1].max():0.5]\n",
    "vvx = np.concatenate(np.dstack((vx, vy)))\n",
    "\n",
    "# calculamos probabilidades -- vienen en escala logarítmica\n",
    "yy = np.exp(gmm.score_samples(vvx))\n",
    "\n",
    "# dibujamos un contorno encima de los datos\n",
    "plt.scatter(peso, estatura, color='lightgray')\n",
    "plt.contour(vx, vy, yy.reshape((vx.shape[0], vy.shape[1])), cmap=\"inferno\", extend=\"both\", linewidths=1.0, levels=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixturas bayesianas variacionales\n",
    "\n",
    "Uno de los problemas clásicos al trabajar con mixturas finitas es determinar el número de components ($K$), un problema parecido al de determinar el número de clusters en *k-means*. En el caso de los modelos de mixturas, uno de los métodos tradicionales es probar con distintos valores de $K$ y buscar un número que reduzca el criterio de información de Akaike (AIC), el cual se puede encontrar con el método ``aic()`` en ``scikit-learn``.\n",
    "\n",
    "Hay métodos más elegantes. Una de ellas es utilizar una **mixtura bayesiana variacional**. En esta, el número de componentes también es un objetivo de inferencia. En ``scikit-learn``, es accesible mediante el método ``sklearn.mixture.BayesianGaussianMixture``. \n",
    "\n",
    "Un **detalle importante** al utilizar estas mixturas es que se debe especificar un número **máximo** de componentes. Adicionalmente, el proceso de inferencia de este modelo de mixtura es mucho más lento ya que debe continuamente re-evaluar el número de componentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "\n",
    "X = np.stack((peso, estatura)).T\n",
    "bgmm = BayesianGaussianMixture(n_components=10, max_iter=1000)\n",
    "bgmm.fit(X)\n",
    "\n",
    "vx, vy = np.mgrid[X[:,0].min():X[:,0].max():0.5, X[:,1].min():X[:,1].max():0.5]\n",
    "vvx = np.concatenate(np.dstack((vx, vy)))\n",
    "\n",
    "# calculamos probabilidades -- vienen en escala logarítmica\n",
    "yy = np.exp(bgmm.score_samples(vvx))\n",
    "\n",
    "# dibujamos un contorno encima de los datos\n",
    "plt.scatter(peso, estatura, color='lightgray')\n",
    "plt.contour(vx, vy, yy.reshape((vx.shape[0], vy.shape[1])), cmap=\"inferno\", extend=\"both\", linewidths=1.0, levels=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando valores de acuerdo a una mixtura\n",
    "\n",
    "Como hemos dicho, además de los parámetros (en el caso en que sean interpretables), el valor de los modelos generativos es el de poder generar contenido o datos de acuerdo al modelo que hemos aprendido. Generalmente este proceso involucra la generación de números aleatorios y luego su \"proyección\" para que estén de acuerdo a la distribución aprendida.\n",
    "\n",
    "En el caso de las mixturas, en ``scikit-learn`` estas se encuentran disponibles bajo el método ``sample``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "\n",
    "X = np.stack((peso, estatura)).T\n",
    "bgmm = BayesianGaussianMixture(n_components=10, max_iter=1000)\n",
    "bgmm.fit(X)\n",
    "\n",
    "# muestreamos y dibujamos\n",
    "mx = bgmm.sample(5000)\n",
    "plt.scatter(mx[0][:,0], mx[0][:,1], color='gray', alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otros tipos de mixturas \n",
    "\n",
    "La mixtura gaussiana es una de las más importantes. Su aplicación a datos numéricos y su versatilidad la hace ubícua. Sin embargo, no es la única. Se puede crear mixturas de cualquier tipo de distribuciones conocidas. Por mencionar algunas:\n",
    "\n",
    "  - **Mixturas Bernoulli** (o Beta-Bernoulli): para datos binarios, p.ej. para datos transaccionales\n",
    "  - **Mixturas Multinomiales** (o Dirichlet-Multinomiales): para datos categóricos, p.ej. para documentos cortos\n",
    "  - **Mixturas de modelos de regresión**: útiles para datos heteroscedásticos\n",
    "  - **Modelos ocultos de Markov (HMMs)**: para secuencias categóricas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
